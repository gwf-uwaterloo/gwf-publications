<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Tracing shapes with eyes - GWF Anthology</title><meta name=generator content="Hugo 0.68.3"><link href=/gwf-publications/gwficon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/gwf-publications/css/main.min.8976777c0832d068a49d330764e507857027f1efa3b8501cf349b0e2db7410fc.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/gwf-publications/css/academicons.min.css><meta content="Tracing shapes with eyes" name=citation_title><meta content="Mohammad Rakib Hasan" name=citation_author><meta content="Debajyoti Mondal" name=citation_author><meta content="Carl Gutwin" name=citation_author><meta content="Proceedings of the 11th Augmented Human International Conference" name=citation_journal_title><meta content="2020" name=citation_publication_date><meta content="https://gwf-uwaterloo.github.io/gwf-publications/G20-89001.pdf" name=citation_pdf_url><meta content="10.1145/3396339.3396390" name=citation_doi><meta property="og:title" content="Tracing shapes with eyes"><meta property="og:image" content="https://gwf-uwaterloo.github.io/gwf-publications/thumb/G20-89001.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/G20-89001"><meta property="og:description" content="Mohammad Rakib Hasan, Debajyoti Mondal, Carl Gutwin. Proceedings of the 11th Augmented Human International Conference. 2020."><link rel=canonical href=https://aclanthology.org/G20-89001></head><body><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/gwf-publications/><img src=/gwf-publications/images/gwf-logo.svg width=56 alt="GWF Logo">
<span class="d-none d-md-inline pl-md-2">GWF Anthology</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a href=https://gwf-uwaterloo.github.io/gwf-publications/G20-89001.pdf>Tracing shapes with eyes</a></h2><p class=lead><a href=/gwf-publications/people/M/Mohammad-Rakib-Hasan/>Mohammad Rakib Hasan</a>,
<a href=/gwf-publications/people/D/Debajyoti-Mondal/>Debajyoti Mondal</a>,
<a href=/gwf-publications/people/C/Carl-Gutwin/>Carl Gutwin</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3"><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Eye tracking systems can provide people with severe motor impairments a way to communicate through gaze-based interactions. Such systems transform a user's gaze input into mouse pointer coordinates that can trigger keystrokes on an on-screen keyboard. However, typing using this approach requires large back-and-forth eye movements, and the required effort depends both on the length of the text and the keyboard layout. Motivated by the idea of sketch-based image search, we explore a gaze-based approach where users draw a shape on a sketchpad using gaze input, and the shape is used to search for similar letters, words, and other predefined controls. The sketch-based approach is area efficient (compared to an on-screen keyboard), allows users to create custom commands, and creates opportunities for gaze-based authentication. Since variation in the drawn shapes makes the search difficult, the system can show a guide (e.g., a 14-segment digital display) on the sketchpad so that users can trace their desired shape. In this paper, we take a first step that investigates the feasibility of the sketch-based approach, by examining how well users can trace a given shape using gaze input. We designed an interface where participants traced a set of given shapes. We then compared the similarity of the drawn and traced shapes. Our study results show the potential of the sketch-based approach: users were able to trace shapes reasonably well using gaze input, even for complex shapes involving three letters; shape tracing accuracy for gaze was better than `free-form' hand drawing. We also report on how different shape complexities influence the time and accuracy of the shape tracing tasks.</span></div></div><dl><dt>Anthology ID:</dt><dd>G20-89001</dd><dt>Volume:</dt><dd><a href=/gwf-publications/volumes/G20-89/>Proceedings of the 11th Augmented Human International Conference</a></dd><dt>Month:</dt><dd></dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd></dd><dt>Venue:</dt><dd><a href=/gwf-publications/venues/gwf/>GWF</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>ACM</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd></dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/G20-89001>https://aclanthology.org/G20-89001</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.1145/3396339.3396390 title="To the current version of the paper by DOI">10.1145/3396339.3396390</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">Hasan-2020-Tracing</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Mohammad Rakib Hasan, Debajyoti Mondal, and Carl Gutwin. 2020. <a href=https://aclanthology.org/G20-89001>Tracing shapes with eyes</a>. <i>Proceedings of the 11th Augmented Human International Conference</i>.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/G20-89001>Tracing shapes with eyes</a> (Hasan et al., GWF 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeBibtexContent><i class="far fa-clipboard pr-2"></i>BibTeX</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeModsContent><i class="far fa-clipboard pr-2"></i>MODS XML</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeEndnoteContent><i class="far fa-clipboard pr-2"></i>Endnote</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://gwf-uwaterloo.github.io/gwf-publications/G20-89001.pdf>https://gwf-uwaterloo.github.io/gwf-publications/G20-89001.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://gwf-uwaterloo.github.io/gwf-publications/G20-89001.pdf title="Open PDF of 'Tracing shapes with eyes'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" title="Open dialog for exporting citations" data-toggle=modal data-target=#citeModal href=#><i class="fas fa-quote-left"></i><span class=pl-2>Cite</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Tracing+shapes+with+eyes" title="Search for 'Tracing shapes with eyes' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=true>BibTeX</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=false>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel><pre id=citeBibtexContent class="bg-light border p-2" style=max-height:50vh>@article{Hasan-2020-Tracing,
    title = &#34;Tracing shapes with eyes&#34;,
    author = &#34;Hasan, Mohammad Rakib  and
      Mondal, Debajyoti  and
      Gutwin, Carl&#34;,
    journal = &#34;Proceedings of the 11th Augmented Human International Conference&#34;,
    year = &#34;2020&#34;,
    publisher = &#34;ACM&#34;,
    url = &#34;https://aclanthology.org/G20-89001&#34;,
    doi = &#34;10.1145/3396339.3396390&#34;,
    abstract = &#34;Eye tracking systems can provide people with severe motor impairments a way to communicate through gaze-based interactions. Such systems transform a user&#39;s gaze input into mouse pointer coordinates that can trigger keystrokes on an on-screen keyboard. However, typing using this approach requires large back-and-forth eye movements, and the required effort depends both on the length of the text and the keyboard layout. Motivated by the idea of sketch-based image search, we explore a gaze-based approach where users draw a shape on a sketchpad using gaze input, and the shape is used to search for similar letters, words, and other predefined controls. The sketch-based approach is area efficient (compared to an on-screen keyboard), allows users to create custom commands, and creates opportunities for gaze-based authentication. Since variation in the drawn shapes makes the search difficult, the system can show a guide (e.g., a 14-segment digital display) on the sketchpad so that users can trace their desired shape. In this paper, we take a first step that investigates the feasibility of the sketch-based approach, by examining how well users can trace a given shape using gaze input. We designed an interface where participants traced a set of given shapes. We then compared the similarity of the drawn and traced shapes. Our study results show the potential of the sketch-based approach: users were able to trace shapes reasonably well using gaze input, even for complex shapes involving three letters; shape tracing accuracy for gaze was better than `free-form&#39; hand drawing. We also report on how different shape complexities influence the time and accuracy of the shape tracing tasks.&#34;,
}
</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/gwf-publications/G20-89001.bib><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeBibtexContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeMods role=tabpanel><pre id=citeModsContent class="bg-light border p-2" style=max-height:50vh>﻿&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;Hasan-2020-Tracing&#34;&gt;
    &lt;titleInfo&gt;
        &lt;title&gt;Tracing shapes with eyes&lt;/title&gt;
    &lt;/titleInfo&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Mohammad&lt;/namePart&gt;
        &lt;namePart type=&#34;given&#34;&gt;Rakib&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Hasan&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Debajyoti&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Mondal&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Carl&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Gutwin&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;originInfo&gt;
        &lt;dateIssued&gt;2020&lt;/dateIssued&gt;
    &lt;/originInfo&gt;
    &lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
    &lt;genre authority=&#34;bibutilsgt&#34;&gt;journal article&lt;/genre&gt;
    &lt;relatedItem type=&#34;host&#34;&gt;
        &lt;titleInfo&gt;
            &lt;title&gt;Proceedings of the 11th Augmented Human International Conference&lt;/title&gt;
        &lt;/titleInfo&gt;
        &lt;originInfo&gt;
            &lt;issuance&gt;continuing&lt;/issuance&gt;
            &lt;publisher&gt;ACM&lt;/publisher&gt;
        &lt;/originInfo&gt;
        &lt;genre authority=&#34;marcgt&#34;&gt;periodical&lt;/genre&gt;
        &lt;genre authority=&#34;bibutilsgt&#34;&gt;academic journal&lt;/genre&gt;
    &lt;/relatedItem&gt;
    &lt;abstract&gt;Eye tracking systems can provide people with severe motor impairments a way to communicate through gaze-based interactions. Such systems transform a user’s gaze input into mouse pointer coordinates that can trigger keystrokes on an on-screen keyboard. However, typing using this approach requires large back-and-forth eye movements, and the required effort depends both on the length of the text and the keyboard layout. Motivated by the idea of sketch-based image search, we explore a gaze-based approach where users draw a shape on a sketchpad using gaze input, and the shape is used to search for similar letters, words, and other predefined controls. The sketch-based approach is area efficient (compared to an on-screen keyboard), allows users to create custom commands, and creates opportunities for gaze-based authentication. Since variation in the drawn shapes makes the search difficult, the system can show a guide (e.g., a 14-segment digital display) on the sketchpad so that users can trace their desired shape. In this paper, we take a first step that investigates the feasibility of the sketch-based approach, by examining how well users can trace a given shape using gaze input. We designed an interface where participants traced a set of given shapes. We then compared the similarity of the drawn and traced shapes. Our study results show the potential of the sketch-based approach: users were able to trace shapes reasonably well using gaze input, even for complex shapes involving three letters; shape tracing accuracy for gaze was better than ‘free-form’ hand drawing. We also report on how different shape complexities influence the time and accuracy of the shape tracing tasks.&lt;/abstract&gt;
    &lt;identifier type=&#34;citekey&#34;&gt;Hasan-2020-Tracing&lt;/identifier&gt;
    &lt;identifier type=&#34;doi&#34;&gt;10.1145/3396339.3396390&lt;/identifier&gt;
    &lt;location&gt;
        &lt;url&gt;https://aclanthology.org/G20-89001&lt;/url&gt;
    &lt;/location&gt;
    &lt;part&gt;
        &lt;date&gt;2020&lt;/date&gt;
    &lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/gwf-publications/G20-89001.xml><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeModsContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeEndnote role=tabpanel><pre id=citeEndnoteContent class="bg-light border p-2" style=max-height:50vh>﻿%0 Journal Article
%T Tracing shapes with eyes
%A Hasan, Mohammad Rakib
%A Mondal, Debajyoti
%A Gutwin, Carl
%J Proceedings of the 11th Augmented Human International Conference
%D 2020
%I ACM
%F Hasan-2020-Tracing
%X Eye tracking systems can provide people with severe motor impairments a way to communicate through gaze-based interactions. Such systems transform a user’s gaze input into mouse pointer coordinates that can trigger keystrokes on an on-screen keyboard. However, typing using this approach requires large back-and-forth eye movements, and the required effort depends both on the length of the text and the keyboard layout. Motivated by the idea of sketch-based image search, we explore a gaze-based approach where users draw a shape on a sketchpad using gaze input, and the shape is used to search for similar letters, words, and other predefined controls. The sketch-based approach is area efficient (compared to an on-screen keyboard), allows users to create custom commands, and creates opportunities for gaze-based authentication. Since variation in the drawn shapes makes the search difficult, the system can show a guide (e.g., a 14-segment digital display) on the sketchpad so that users can trace their desired shape. In this paper, we take a first step that investigates the feasibility of the sketch-based approach, by examining how well users can trace a given shape using gaze input. We designed an interface where participants traced a set of given shapes. We then compared the similarity of the drawn and traced shapes. Our study results show the potential of the sketch-based approach: users were able to trace shapes reasonably well using gaze input, even for complex shapes involving three letters; shape tracing accuracy for gaze was better than ‘free-form’ hand drawing. We also report on how different shape complexities influence the time and accuracy of the shape tracing tasks.
%R 10.1145/3396339.3396390
%U https://aclanthology.org/G20-89001
%U https://doi.org/10.1145/3396339.3396390

</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/gwf-publications/G20-89001.endf><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeEndnoteContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Tracing shapes with eyes](https://aclanthology.org/G20-89001) (Hasan et al., GWF 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/G20-89001>Tracing shapes with eyes</a> (Hasan et al., GWF 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Mohammad Rakib Hasan, Debajyoti Mondal, and Carl Gutwin. 2020. <a href=https://aclanthology.org/G20-89001>Tracing shapes with eyes</a>. <i>Proceedings of the 11th Augmented Human International Conference</i>.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div></section></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class=container><p class="text-muted small px-1">Global Water Futures Publications!</p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script><script>$(function(){$('[data-toggle="tooltip"]').tooltip();if($("#toggle-all-abstracts")){$("#toggle-all-abstracts").click(function(){var target=$("#toggle-all-abstracts");target.attr("disabled",true);if(target.attr("data-toggle-state")=="hide"){$(".abstract-collapse").collapse('show');target.attr("data-toggle-state","show");}else{$(".abstract-collapse").collapse('hide');target.attr("data-toggle-state","hide");}
target.attr("disabled",false);});$("#toggle-all-abstracts").attr("disabled",false);}})</script><script src=/gwf-publications/js/clipboard.min.js></script><script>$(document).ready(function(){if(ClipboardJS.isSupported()){success_fn=function(e){var src=$(e.trigger);src.toggleClass("btn-success");src.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check");e.clearSelection();setTimeout(function(){src.toggleClass("btn-success");src.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check");},2000);};var clipboard=new ClipboardJS(".btn-clipboard");clipboard.on('success',success_fn);$(".btn-clipboard").removeClass("d-none");var clipboard_outside=new ClipboardJS(".btn-clipboard-outside",{text:function(trigger){var target=trigger.getAttribute("data-clipboard-target");return $(target).text();}});clipboard_outside.on('success',success_fn);$(".btn-clipboard-outside").removeClass("d-none");}});</script></body></html>