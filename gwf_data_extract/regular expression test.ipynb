{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a605245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "import urllib.parse\n",
    "import difflib\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as et\n",
    "from zlib import crc32 \n",
    "import yaml\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import xml.dom.minidom as md\n",
    "import html\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa140d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_dois():\n",
    "    all_website_dois = []\n",
    "    for file in os.listdir('../data/xml/'):        \n",
    "        input = '../data/xml/'+ file        \n",
    "        tree = et.parse(input)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for paper in root.iter('paper'):\n",
    "            all_website_dois.append(paper.find(\"doi\").text)\n",
    "\n",
    "    return all_website_dois\n",
    "\n",
    "a = get_website_dois()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259edef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(a).to_csv('all_doi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "16199da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import html\n",
    "\n",
    "# print(os.listdir('data/xml/'))\n",
    "\n",
    "def get_website_dois():\n",
    "    all_website_dois = []\n",
    "    for file in os.listdir('../data/xml/'):\n",
    "        # sheet = pd.DataFrame(columns=[\"title\", \"author\", \"abstract\", \"url\", \"pages\", \"doi\", \"bibkey\"])    \n",
    "        input = '../data/xml/'+ file\n",
    "        # output = 'gwf_data_extract/'+file+'all_website_dois.csv'\n",
    "        tree = et.parse(input)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for paper in root.iter('paper'):\n",
    "            all_website_dois.append(paper.find(\"doi\").text)\n",
    "\n",
    "    return all_website_dois\n",
    "\n",
    "def get_website_bibkeys():\n",
    "    all_website_bibkeys = []\n",
    "    files_path = '../data/xml/'\n",
    "    for file in os.listdir(files_path):\n",
    "        input = files_path+ file        \n",
    "        tree = et.parse(input)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for paper in root.iter('paper'):\n",
    "            all_website_bibkeys.append(paper.find(\"bibkey\").text)\n",
    "\n",
    "    return all_website_bibkeys\n",
    "\n",
    "def find_doi_diffs(all_to_compare_file):\n",
    "    dois = pd.read_csv(all_to_compare_file, header=0).applymap(str.lower).fillna(\"\").values.tolist()\n",
    "    dois = [item[0] for item in dois]\n",
    "    websit_dois = get_website_dois()\n",
    "    print(len(dois))\n",
    "    print(len(websit_dois))\n",
    "\n",
    "    added_dois = [x for x in dois if x not in websit_dois]\n",
    "    # added_dois = [x for x in websit_dois if x not in dois]\n",
    "    return added_dois\n",
    "\n",
    "\n",
    "# a = find_doi_diffs('doi_from_USask.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bf9d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1001/hyp.14465\n",
      "404\n",
      "10.1002/esp.4833\n",
      "10.1002/esp.4919\n",
      "10.1002/esp.5140\n",
      "10.1002/etc.4852\n",
      "10.1002/etc.5200\n",
      "10.1002/hyp.10905\n",
      "10.1002/hyp.10906\n",
      "10.1002/hyp.10910\n",
      "10.1002/hyp.11628\n",
      "10.1002/hyp.13622\n",
      "10.1002/hyp.13915\n",
      "10.1002/hyp.13960\n",
      "10.1002/hyp.14038\n",
      "10.1002/hyp.14112\n",
      "10.1002/hyp.14127\n",
      "10.1002/hyp.14410\n",
      "10.1002/hyp.14501\n",
      "10.1002/jeq2.20155\n",
      "10.1002/joc.6571\n",
      "10.1002/ppp.2051\n",
      "10.1002/rcm.8995\n",
      "10.1002/smll.202101171\n",
      "10.1002/vzj2.20043\n",
      "10.1002/vzj2.20101\n",
      "10.1002/wcc.688\n",
      "10.1007/s10460-020-10059-z\n",
      "10.1007/s00382-020-05388-y\n",
      "10.1007/s00484-020-01867-3\n",
      "10.1007/s00704-021-03791-x\n",
      "10.1007/s10584-021-02956-x\n",
      "10.1007/s10661-020-08570-1\n",
      "10.1007/s10664-020-09863-2\n",
      "10.1007/s10933-020-00162-w\n",
      "10.1007/s11069-020-04425-7\n",
      "10.1007/s11273-022-09857-5\n",
      "10.1007/s11625-020-00901-y\n",
      "10.1007/s12665-016-6027-1\n",
      "10.1007/s13679-020-00399-6\n",
      "10.1016/b978-0-12-409548-9.12441-8\n",
      "10.1016/j.aca.2020.12.056\n",
      "10.1016/j.agrformet.2019.107774\n",
      "10.1016/j.agrformet.2020.107910\n",
      "10.1016/j.agrformet.2021.108407\n",
      "10.1016/j.apgeochem.2021.105053\n",
      "10.1016/j.aquatox.2020.105616\n",
      "10.1016/j.aquatox.2020.105658\n",
      "10.1016/j.atmosres.2020.104973\n",
      "10.1016/j.chemosphere.2020.129195\n",
      "10.1016/j.coldregions.2019.102918\n",
      "10.1016/j.coldregions.2020.103218\n",
      "10.1016/j.dib.2021.106723\n",
      "10.1016/j.ecolind.2020.106684\n",
      "10.1016/j.eehl.2022.06.002\n",
      "10.1016/j.ejrh.2020.100754\n",
      "10.1016/j.ejrh.2021.100978\n",
      "10.1016/j.envres.2020.109112\n",
      "10.1016/j.envsoft.2020.104829\n",
      "10.1016/j.envsoft.2020.104885\n",
      "10.1016/j.envsoft.2020.104888\n",
      "10.1016/j.envsoft.2021.104997\n",
      "10.1016/j.envsoft.2021.105025\n",
      "10.1016/j.envsoft.2021.105282\n",
      "10.1016/j.geoderma.2020.114630\n",
      "10.1016/j.geoderma.2022.115827\n",
      "10.1016/j.hydrol.2021.126793\n",
      "404\n",
      "10.1016/j.ijdrr.2020.102001\n",
      "10.1016/j.jece.2021.105305\n",
      "10.1016/j.jenvman.2021.112262\n",
      "10.1016/j.jenvman.2021.112336\n",
      "10.1016/j.jhydrol.2020.125735\n",
      "10.1016/j.jhydrol.2020.125744\n",
      "10.1016/j.jhydrol.2020.125752\n",
      "10.1016/j.jhydrol.2020.125846\n",
      "10.1016/j.jhydrol.2021.126268\n",
      "10.1016/j.jss.2020.110686\n",
      "10.1016/j.marpol.2020.104150\n",
      "10.1016/j.mex.2022.101627\n",
      "10.1016/j.mtcomm.2022.103132\n",
      "10.1016/j.rse.2020.111915\n",
      "10.1016/j.rse.2020.112206\n",
      "10.1016/j.rse.2021.112732\n",
      "10.1016/j.scitotenv.2018.12.077\n",
      "10.1016/j.scitotenv.2020.143648\n",
      "10.1016/j.scitotenv.2020.144612\n",
      "10.1016/j.scitotenv.2021.146698\n",
      "10.1016/j.scitotenv.2021.149628\n",
      "10.1016/j.scitotenv.2021.151060\n",
      "10.1016/j.snb.2020.129023\n",
      "10.1016/j.wace.2020.100290\n",
      "10.1016/j.wace.2022.100441\n",
      "10.1016/j.watres.2020.116560\n",
      "10.1017/jog.2021.101\n",
      "10.1021/acs.est.0c02522\n",
      "10.1021/acs.est.0c02838\n",
      "10.1021/acs.est.0c03931\n",
      "10.1021/acs.est.0c06450\n",
      "10.1021/acs.est.9b07178\n",
      "10.1021/acs.iecr.1c02914\n",
      "10.1029/2018wr023403\n",
      "10.1029/2019jg005389\n",
      "10.1029/2019wr025975\n",
      "10.1029/2020ef001667\n",
      "10.1029/2020ef001824\n",
      "10.1029/2020gb006626\n",
      "10.1029/2020gl088000\n",
      "10.1029/2020gl088288\n",
      "10.1029/2020gl089829\n",
      "10.1029/2020jd032696\n",
      "10.1029/2020jd033577\n",
      "10.1029/2020jd034118\n",
      "10.1029/2020jg005713\n",
      "10.1029/2020jg005774\n",
      "10.1029/2020jg005833\n",
      "10.1029/2020wr027343\n",
      "10.1029/2020wr027447\n",
      "10.1029/2020wr027466\n",
      "10.1029/2020wr028096\n",
      "10.1029/2021gl092658\n",
      "10.1029/tr002i001p00002\n",
      "10.1038/521032c\n",
      "10.1038/s41558-020-00920-8\n",
      "10.1038/s41586-020-03042-5\n",
      "10.1038/s41597-020-0534-3\n",
      "10.1038/s41598-021-02606-3\n",
      "10.1038/s43247-021-00184-w\n",
      "10.1061/(asce)he.1943-5584.0002003\n",
      "10.1080/01490451.2021.1998256\n",
      "10.1080/02626667.2018.1518626\n",
      "10.1080/02626667.2020.1751847\n",
      "10.1080/07011784.2020.1758215\n",
      "10.1080/07038992.2021.1900717\n",
      "10.1080/13658816.2021.1931236\n",
      "10.1088/1748-9326/aa70cb\n",
      "10.1088/1748-9326/ab9967\n",
      "10.1088/1748-9326/abab34\n",
      "10.1088/1748-9326/abb55f\n",
      "10.1088/2515-7620/ac53c2\n",
      "10.1093/obo/9780199363445-0115\n",
      "10.1101/2021.02.22.21252041\n",
      "10.1109/jiot.2020.3044526\n",
      "10.1109/jsen.2020.2978758\n",
      "10.1109/jstars.2019.2920676\n",
      "10.1109/saner53432.2022.00152\n",
      "10.1111/2041-210x.13756\n",
      "10.1111/cag.12598\n",
      "10.1111/caje.12560\n",
      "10.1111/gcb.14905\n",
      "10.1111/gcb.15353\n",
      "10.1111/gean.12228\n",
      "10.1111/jfr3.12697\n",
      "10.1111/nph.16479\n",
      "10.1111/wej.12634\n",
      "10.1117/12.2601682\n",
      "10.1126/sciadv.aay8558\n",
      "10.1126/science.abd5942\n",
      "10.1139/as-2020-0006\n",
      "10.1139/cjfas-2020-0141\n",
      "10.1139/er-2019-0063\n",
      "10.1139/facets-2020-0022\n",
      "10.1139/facets-2020-0031\n",
      "10.1142/s2345737621310023\n",
      "10.1146/annurev-environ-012220-125703\n",
      "10.1175/bams-d-19-0001.1\n",
      "10.1175/bams-d-19-0329.1\n",
      "10.1175/jcli-d-19-1013.1\n",
      "10.1175/jhm-d-15-0226.1\n",
      "10.1175/jhm-d-17-0245.1\n",
      "10.1175/jhm-d-20-0034.1\n",
      "10.1175/jhm-d-20-0040.1\n",
      "10.1175/jhm-d-20-0096.1\n",
      "10.1175/jhm-d-20-0131.1\n",
      "10.1177/0959683620972792\n",
      "10.14430/arctic70869\n",
      "10.15402/esj.v6i1.68178\n",
      "10.20383/101.0275\n",
      "404\n",
      "10.22004/ag.econ.302498\n",
      "10.22541/au.158880248.84807120\n",
      "10.24908/cpp-apc.v2021i01.13176\n",
      "10.3389/feart.2020.00278\n",
      "10.3389/fendo.2021.611281\n",
      "10.3389/fenvs.2020.556452\n",
      "10.3389/fenvs.2022.915329\n",
      "10.3390/agronomy10081112\n",
      "10.3390/app10217937\n",
      "10.3390/app10238364\n",
      "10.3390/atmos10110682\n",
      "10.3390/atmos11090937\n",
      "10.3390/f11030304\n",
      "10.3390/f11080829\n",
      "10.3390/hydrology6040092\n",
      "10.3390/ijgi10040207\n",
      "10.3390/plants9101295\n",
      "10.3390/rs11172033\n",
      "10.3390/rs12101543\n",
      "10.3390/rs12182904\n",
      "10.3390/rs12183101\n",
      "10.3390/rs13030333\n",
      "10.3390/rs13040806\n",
      "10.3390/su11216178\n",
      "10.3390/w12123353\n",
      "10.3390/w12123370\n",
      "10.48550/arxiv.2204.05345\n",
      "404\n",
      "10.5194/bg-17-3563-2020\n",
      "10.5194/bg-17-361-2020\n",
      "10.5194/bg-17-4261-2020\n",
      "10.5194/esd-11-301-2020\n",
      "10.5194/essd-12-1973-2020\n",
      "10.5194/essd-13-1335-2021\n",
      "10.5194/essd-13-2875-2021\n",
      "10.5194/essd-13-5127-2021\n",
      "10.5194/essd-2020-303\n",
      "10.5194/essd-2021-68\n",
      "10.5194/gi-9-293-2020\n",
      "10.5194/gmd-13-1827-2020\n",
      "10.5194/gmd-13-5401-2020\n",
      "10.5194/gmd-14-1657-2021\n",
      "10.5194/gmd-14-3295-2021\n",
      "10.5194/gmd-2020-317\n",
      "10.5194/gmd-2022-16\n",
      "10.5194/hess-2020-335\n",
      "10.5194/hess-2021-437\n",
      "10.5194/hess-22-1157-2018\n",
      "10.5194/hess-22-1593-2018\n",
      "10.5194/hess-23-4933-2019\n",
      "10.5194/hess-24-2083-2020\n",
      "10.5194/hess-24-2141-2020\n",
      "10.5194/hess-24-2731-2020\n",
      "10.5194/hess-24-3077-2020\n",
      "10.5194/hess-25-1117-2021\n",
      "10.5194/hess-25-2869-2021\n",
      "10.5194/hess-25-4455-2021\n",
      "10.5194/hess-25-527-2021\n",
      "10.5194/hess-25-5425-2021\n",
      "10.5194/hess-26-2131-2022\n",
      "10.5194/tc-12-759-2018\n",
      "10.5194/tc-14-2795-2020\n",
      "10.5194/tc-14-2925-2020\n",
      "10.5194/tc-14-4279-2020\n",
      "10.5194/tc-14-4687-2020\n",
      "10.5194/tc-14-565-2020\n",
      "10.5194/tc-15-1343-2021\n",
      "10.5194/tc-15-3949-2021\n",
      "10.5194/tc-15-743-2021\n",
      "10.5194/tc-2020-248\n",
      "10.5194/tc-2020-289\n",
      "10.5194/tc-2022-38\n",
      "10.5558/tfc2016-010\n",
      "10.5683/sp2/zrgxq5\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "data_url =('https://api.crossref.org/works/{DOI}')\n",
    "dois = pd.read_csv('extra_publication_cleaned.csv', header=None).iloc[:,0].values.tolist()\n",
    "\n",
    "with open('result.json', 'r') as openfile:\n",
    "    detail_dict = json.load(openfile)\n",
    "\n",
    "for doi in dois:\n",
    "    if doi not in detail_dict:\n",
    "        try:\n",
    "            print(doi)\n",
    "            url = data_url.replace(\"{DOI}\", doi)\n",
    "            r = requests.get(url)\n",
    "            if r.status_code != 200:\n",
    "                print(r.text)\n",
    "\n",
    "            data = r.json()\n",
    "            detail_dict[data['message']['DOI']] = data['message']\n",
    "        except: \n",
    "            print('exception')\n",
    "            pass\n",
    "\n",
    "with open('result_added.json', \"w\") as outfile:\n",
    "    outfile.write(json.dumps(detail_dict, indent=4)) \n",
    "\n",
    "\n",
    "with open('abstract.json', 'r') as openfile:\n",
    "    abs_dict = json.load(openfile)\n",
    "\n",
    "data_url='https://api.openalex.org/works/https://doi.org/{DOI}'\n",
    "for doi in dois:\n",
    "    if doi not in abs_dict:    \n",
    "        r = requests.get(url = data_url.replace(\"{DOI}\", doi))\n",
    "        print(doi)\n",
    "        if r.status_code != 200:\n",
    "            print(r.status_code)\n",
    "        else:\n",
    "            response = r.json()\n",
    "            if response['abstract_inverted_index']:   \n",
    "                abstract = {}         \n",
    "                for key, indexes in response['abstract_inverted_index'].items():\n",
    "                    for index in indexes:\n",
    "                        abstract[index] = key\n",
    "                \n",
    "                abs_dict[response['doi'].replace('https://doi.org/', '')] = ' '.join(list(collections.OrderedDict(sorted(abstract.items())).values()))\n",
    "\n",
    "with open('abstract_added.json', \"w\") as outfile:\n",
    "    outfile.write(json.dumps(abs_dict, indent=4))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4cc4eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'volume' at 0x000001F32435E680>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "007de8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 2\n",
      "2019 5\n",
      "2020 1\n",
      "2017 1\n",
      "2022 5\n",
      "2021 4\n"
     ]
    }
   ],
   "source": [
    "for year, year_dict in paper_dict.items():\n",
    "    print(year,len(year_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3dbcfa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = et.parse(\"G19.xml\")\n",
    "root = tree.getroot()   \n",
    "len(root.findall(\"./volume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "32bde53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'196'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.findall(\"./volume\")[-1].get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "06d99cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vol.getchildren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c4448350",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_folder = os.path.abspath(\"\")\n",
    "\n",
    "def compute_hash(value: bytes) -> str:\n",
    "    checksum = crc32(value) & 0xFFFFFFFF\n",
    "    return f\"{checksum:08x}\"\n",
    "\n",
    "def generate_bibkey(title, author, year, bibkey_list):\n",
    "    author_list = [(item['family'] if 'family' in item else '') for item in author] + [(item['given'] if 'given' in item else '') for item in author]    \n",
    "    title_list = title.split()\n",
    "    count=1\n",
    "    while True:\n",
    "        bibkey = '-'.join(author_list[:count] + [str(year)] + title_list[:count])\n",
    "        if bibkey not in bibkey_list: return bibkey\n",
    "        count += 1\n",
    "\n",
    "script_dir = os.path.abspath(\"\")\n",
    "website_dois = get_website_dois()\n",
    "dois = pd.read_csv(script_dir+'/non_doi_titles.csv', header=None).iloc[:,0].values.tolist()\n",
    "dois = [item.lower() for item in dois if item not in website_dois] # drop DOIs that already exist\n",
    "\n",
    "with open(script_dir+'/result.json', 'r') as openfile:     \n",
    "    json_object = json.load(openfile)\n",
    "\n",
    "with open(script_dir+'/abstract.json', 'r') as openfile:     \n",
    "    abstract_dict = json.load(openfile)\n",
    "\n",
    "paper_dict = {}\n",
    "json_object = {key:item for key, item in json_object.items() if key in dois}\n",
    "json_object = {key: item for key, item in json_object.items() if ('author' in item) and (item['title'])} # drop papers without title or author \n",
    "    \n",
    "for key, item in json_object.items():\n",
    "    try:\n",
    "        year = item['published']['date-parts'][0][0]\n",
    "        journal = item['container-title'][0] if item['container-title'] else ' '\n",
    "        volume = item['volume'] if 'volume' in item else ''\n",
    "        issue = item['issue'] if 'issue' in item else ''\n",
    "        pub_key = 'journal: '+journal+' volume: '+volume+' issue: '+issue\n",
    "        if year not in paper_dict: paper_dict[year]={}\n",
    "        if pub_key not in paper_dict[year]: paper_dict[year][pub_key]={}\n",
    "        paper_dict[year][pub_key][key] = item     \n",
    "    except:            \n",
    "        pass\n",
    "\n",
    "bibkey_list = get_website_bibkeys()\n",
    "\n",
    "for year, year_dict in paper_dict.items():\n",
    "    tree = et.parse(\"G\"+str(year)[-2:]+\".xml\")\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for index, (volume, item) in enumerate(year_dict.items()):\n",
    "        first_item = list(item.items())[0][1]\n",
    "        booktitle = (first_item['container-title'][0] if first_item['container-title'] else ' ')+((', Volume '+first_item['volume']) if 'volume' in first_item else '')+((', Issue '+first_item['issue']) if 'issue' in first_item else '')\n",
    "\n",
    "        tag_vol = root.find('.//booktitle[.=\"'+booktitle+'\"]/....')\n",
    "        if tag_vol: # check if volume has existed\n",
    "            paper_id_ofs = int(tag_vol.findall(\"./paper\")[-1].get('id'))\n",
    "        else:\n",
    "            tag_vol = et.Element(\"volume\")\n",
    "            root.append(tag_vol)\n",
    "            tag_vol.set('id', str(int(root.findall(\"./volume\")[-2].get('id'))+1))\n",
    "\n",
    "            tag_meta = et.Element(\"meta\")\n",
    "            tag_vol.append(tag_meta)\n",
    "            tag_subelement = et.SubElement(tag_meta, \"booktitle\")\n",
    "            tag_subelement.text = booktitle\n",
    "            tag_subelement = et.SubElement(tag_meta, \"publisher\")\n",
    "            tag_subelement.text = first_item['publisher']\n",
    "            tag_subelement = et.SubElement(tag_meta, \"address\")\n",
    "            tag_subelement.text = \"\"\n",
    "            tag_subelement = et.SubElement(tag_meta, \"year\")\n",
    "            tag_subelement.text = str(year)\n",
    "            paper_id_ofs = 0\n",
    "\n",
    "        tmep_dict = year_dict[volume]\n",
    "        for idx, (key, tmep_dict_item) in enumerate(tmep_dict.items()):\n",
    "            skip_flag = 0\n",
    "            for author in tmep_dict_item['author']: # skip papers that don't have author name or family\n",
    "                if ('given' not in author) or ('family' not in author): skip_flag = 1\n",
    "            if skip_flag: continue\n",
    "        \n",
    "            tag_paper = et.Element(\"paper\")\n",
    "            tag_vol.append(tag_paper)\n",
    "            tag_paper.set('id', str(idx+1+paper_id_ofs))\n",
    "            tag_subelement = et.SubElement(tag_paper, \"title\")\n",
    "            tag_subelement.text = tmep_dict_item['title'][0]\n",
    "            for author in tmep_dict_item['author']:\n",
    "                tag_subelement = et.SubElement(tag_paper, \"author\")\n",
    "                tag_subsubelement = et.SubElement(tag_subelement, \"first\")\n",
    "                tag_subsubelement.text = (author['given'] if 'given' in author else '')\n",
    "                tag_subsubelement = et.SubElement(tag_subelement, \"last\")\n",
    "                tag_subsubelement.text = (author['family'] if 'family' in author else '')\n",
    "\n",
    "            tag_subelement = et.SubElement(tag_paper, \"abstract\")\n",
    "            tag_subelement.text = abstract_dict[tmep_dict_item['DOI']] if tmep_dict_item['DOI'] in abstract_dict else ''\n",
    "            tag_subelement = et.SubElement(tag_paper, \"url\")\n",
    "            tag_subelement.text =  f'G{str(year)[-2:]}-{index+1}{(idx+1):03}'\n",
    "            tag_subelement.set('hash', compute_hash(str.encode(tag_subelement.text)))\n",
    "        \n",
    "            if 'page' in tmep_dict_item:\n",
    "                tag_subelement = et.SubElement(tag_paper, \"pages\")\n",
    "                tag_subelement.text = tmep_dict_item['page']\n",
    "\n",
    "            tag_subelement = et.SubElement(tag_paper, \"doi\")\n",
    "            tag_subelement.text = tmep_dict_item['DOI']\n",
    "            tag_subelement = et.SubElement(tag_paper, \"bibkey\")\n",
    "            bibkey = generate_bibkey(tmep_dict_item['title'][0], tmep_dict_item['author'], year, bibkey_list)\n",
    "            tag_subelement.text = bibkey\n",
    "            bibkey_list.append(bibkey)\n",
    "\n",
    "    tree = et.ElementTree(root)    \n",
    "    file_name = \"G\"+str(year)[-2:]+\"a.xml\"   \n",
    "    with open(file_name, \"w\", encoding=\"UTF-8\") as f:\n",
    "        f.write(etree.tostring(etree.XML(et.tostring(root, encoding=\"UTF-8\", xml_declaration=True), parser=etree.XMLParser(remove_blank_text=True))).decode())\n",
    "\n",
    "    xml_pretty_str = md.parse(file_name)\n",
    "    xml_pretty_str = xml_pretty_str.toprettyxml(encoding='UTF-8').decode()\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(xml_pretty_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "81e8c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ce2962d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "9a645b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('name_variants.yaml', 'r') as file:\n",
    "    dict_file = yaml.safe_load(file)\n",
    "for key in json_object:\n",
    "    for item in json_object[key]['author']:\n",
    "        if ('given' in item) and ('family' in item): element = {'canonical' : {'first': (item['given'] if 'given' in item else ''), 'last': (item['family'] if 'family' in item else '')}, 'id':(item['given'] if 'given' in item else '').replace('.', '')+'-'+(item['family'] if 'family' in item else '').replace('.', '')}\n",
    "        if element not in dict_file: dict_file.append(element)\n",
    "\n",
    "with open('name_variants.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_file, file, default_flow_style=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "75fd1acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prime_service[5] in dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e46db9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'canonical': {'first': 'Grant', 'last': 'Ferguson'}, 'id': 'Grant-Ferguson'},\n",
       " {'canonical': {'first': 'Jennifer C.', 'last': 'McIntosh'},\n",
       "  'id': 'Jennifer C-McIntosh'},\n",
       " {'canonical': {'first': 'Stephen E.', 'last': 'Grasby'},\n",
       "  'id': 'Stephen E-Grasby'},\n",
       " {'canonical': {'first': 'M. Jim', 'last': 'Hendry'}, 'id': 'M Jim-Hendry'},\n",
       " {'canonical': {'first': 'Scott', 'last': 'Jasechko'}, 'id': 'Scott-Jasechko'},\n",
       " {'canonical': {'first': 'Matthew B. J.', 'last': 'Lindsay'},\n",
       "  'id': 'Matthew B J-Lindsay'},\n",
       " {'canonical': {'first': 'Elco', 'last': 'Luijendijk'},\n",
       "  'id': 'Elco-Luijendijk'},\n",
       " {'canonical': {'first': 'Serghei A.', 'last': 'Bocaniov'},\n",
       "  'id': 'Serghei A-Bocaniov'},\n",
       " {'canonical': {'first': 'Philippe', 'last': 'Van Cappellen'},\n",
       "  'id': 'Philippe-Van Cappellen'},\n",
       " {'canonical': {'first': 'Donald', 'last': 'Scavia'}, 'id': 'Donald-Scavia'},\n",
       " {'canonical': {'first': 'Laura E.', 'last': 'Condon'},\n",
       "  'id': 'Laura E-Condon'},\n",
       " {'canonical': {'first': 'Katherine H.', 'last': 'Markovich'},\n",
       "  'id': 'Katherine H-Markovich'},\n",
       " {'canonical': {'first': 'Christa A.', 'last': 'Kelleher'},\n",
       "  'id': 'Christa A-Kelleher'},\n",
       " {'canonical': {'first': 'Jeffrey J.', 'last': 'McDonnell'},\n",
       "  'id': 'Jeffrey J-McDonnell'},\n",
       " {'canonical': {'first': 'Kevin R.', 'last': 'Green'}, 'id': 'Kevin R-Green'},\n",
       " {'canonical': {'first': 'Tanner A.', 'last': 'Bohn'}, 'id': 'Tanner A-Bohn'},\n",
       " {'canonical': {'first': 'Raymond J.', 'last': 'Spiteri'},\n",
       "  'id': 'Raymond J-Spiteri'},\n",
       " {'canonical': {'first': 'Yanping', 'last': 'Li'}, 'id': 'Yanping-Li'},\n",
       " {'canonical': {'first': 'Kit', 'last': 'Szeto'}, 'id': 'Kit-Szeto'},\n",
       " {'canonical': {'first': 'Ronald E.', 'last': 'Stewart'},\n",
       "  'id': 'Ronald E-Stewart'},\n",
       " {'canonical': {'first': 'Julie M.', 'last': 'Thériault'},\n",
       "  'id': 'Julie M-Thériault'},\n",
       " {'canonical': {'first': 'Liang', 'last': 'Chen'}, 'id': 'Liang-Chen'},\n",
       " {'canonical': {'first': 'Bohdan', 'last': 'Kochtubajda'},\n",
       "  'id': 'Bohdan-Kochtubajda'},\n",
       " {'canonical': {'first': 'Anthony', 'last': 'Liu'}, 'id': 'Anthony-Liu'},\n",
       " {'canonical': {'first': 'Sudesh', 'last': 'Boodoo'}, 'id': 'Sudesh-Boodoo'},\n",
       " {'canonical': {'first': 'Ron', 'last': 'Goodson'}, 'id': 'Ron-Goodson'},\n",
       " {'canonical': {'first': 'Curtis', 'last': 'Mooney'}, 'id': 'Curtis-Mooney'},\n",
       " {'canonical': {'first': 'Sopan', 'last': 'Kurkute'}, 'id': 'Sopan-Kurkute'},\n",
       " {'canonical': {'first': 'Karl-Erich', 'last': 'Lindenschmidt'},\n",
       "  'id': 'Karl-Erich-Lindenschmidt'},\n",
       " {'canonical': {'first': 'Zhaoqin', 'last': 'Li'}, 'id': 'Zhaoqin-Li'},\n",
       " {'canonical': {'first': 'Igor', 'last': 'Markelov'}, 'id': 'Igor-Markelov'},\n",
       " {'canonical': {'first': 'Raoul‐Marie', 'last': 'Couture'},\n",
       "  'id': 'Raoul‐Marie-Couture'},\n",
       " {'canonical': {'first': 'Rachele', 'last': 'Fischer'},\n",
       "  'id': 'Rachele-Fischer'},\n",
       " {'canonical': {'first': 'Sigrid', 'last': 'Haande'}, 'id': 'Sigrid-Haande'},\n",
       " {'canonical': {'first': 'Prabin', 'last': 'Rokaya'}, 'id': 'Prabin-Rokaya'},\n",
       " {'canonical': {'first': 'Howard', 'last': 'Wheater'}, 'id': 'Howard-Wheater'},\n",
       " {'canonical': {'first': 'Christena', 'last': 'Watts'},\n",
       "  'id': 'Christena-Watts'},\n",
       " {'canonical': {'first': 'Jianxian', 'last': 'Sun'}, 'id': 'Jianxian-Sun'},\n",
       " {'canonical': {'first': 'Paul D.', 'last': 'Jones'}, 'id': 'Paul D-Jones'},\n",
       " {'canonical': {'first': 'Hui', 'last': 'Peng'}, 'id': 'Hui-Peng'},\n",
       " {'canonical': {'first': 'John P.', 'last': 'Giesy'}, 'id': 'John P-Giesy'},\n",
       " {'canonical': {'first': 'Chelene C.', 'last': 'Hanes'},\n",
       "  'id': 'Chelene C-Hanes'},\n",
       " {'canonical': {'first': 'Mike', 'last': 'Wotton'}, 'id': 'Mike-Wotton'},\n",
       " {'canonical': {'first': 'Douglas G.', 'last': 'Woolford'},\n",
       "  'id': 'Douglas G-Woolford'},\n",
       " {'canonical': {'first': 'David L.', 'last': 'Martell'},\n",
       "  'id': 'David L-Martell'},\n",
       " {'canonical': {'first': 'Mike', 'last': 'Flannigan'}, 'id': 'Mike-Flannigan'},\n",
       " {'canonical': {'first': 'Vinay', 'last': 'Patel'}, 'id': 'Vinay-Patel'},\n",
       " {'canonical': {'first': 'Peter', 'last': 'Kruse'}, 'id': 'Peter-Kruse'},\n",
       " {'canonical': {'first': 'P. Ravi', 'last': 'Selvaganapathy'},\n",
       "  'id': 'P Ravi-Selvaganapathy'},\n",
       " {'canonical': {'first': 'Scott N.', 'last': 'Williamson'},\n",
       "  'id': 'Scott N-Williamson'},\n",
       " {'canonical': {'first': 'Brian', 'last': 'Menounos'}, 'id': 'Brian-Menounos'},\n",
       " {'canonical': {'first': 'Mazda', 'last': 'Kompanizare'},\n",
       "  'id': 'Mazda-Kompanizare'},\n",
       " {'canonical': {'first': 'Richard M.', 'last': 'Petrone'},\n",
       "  'id': 'Richard M-Petrone'},\n",
       " {'canonical': {'first': 'Merrin L.', 'last': 'Macrae'},\n",
       "  'id': 'Merrin L-Macrae'},\n",
       " {'canonical': {'first': 'Kevin', 'last': 'De Haan'}, 'id': 'Kevin-De Haan'},\n",
       " {'canonical': {'first': 'Myroslava', 'last': 'Khomik'},\n",
       "  'id': 'Myroslava-Khomik'},\n",
       " {'canonical': {'first': 'Mohamad Amin', 'last': 'Halali'},\n",
       "  'id': 'Mohamad Amin-Halali'},\n",
       " {'canonical': {'first': 'Charles-François', 'last': 'de Lannoy'},\n",
       "  'id': 'Charles-François-de Lannoy'},\n",
       " {'canonical': {'first': 'Cherie J.', 'last': 'Westbrook'},\n",
       "  'id': 'Cherie J-Westbrook'},\n",
       " {'canonical': {'first': 'Erika', 'last': 'Gonzalez‐Akre'},\n",
       "  'id': 'Erika-Gonzalez‐Akre'},\n",
       " {'canonical': {'first': 'Camille', 'last': 'Piponiot'},\n",
       "  'id': 'Camille-Piponiot'},\n",
       " {'canonical': {'first': 'Mauro', 'last': 'Lepore'}, 'id': 'Mauro-Lepore'},\n",
       " {'canonical': {'first': 'Valentine', 'last': 'Herrmann'},\n",
       "  'id': 'Valentine-Herrmann'},\n",
       " {'canonical': {'first': 'James A.', 'last': 'Lutz'}, 'id': 'James A-Lutz'},\n",
       " {'canonical': {'first': 'Jennifer L.', 'last': 'Baltzer'},\n",
       "  'id': 'Jennifer L-Baltzer'},\n",
       " {'canonical': {'first': 'Christopher W.', 'last': 'Dick'},\n",
       "  'id': 'Christopher W-Dick'},\n",
       " {'canonical': {'first': 'Gregory S.', 'last': 'Gilbert'},\n",
       "  'id': 'Gregory S-Gilbert'},\n",
       " {'canonical': {'first': 'Fangliang', 'last': 'He'}, 'id': 'Fangliang-He'},\n",
       " {'canonical': {'first': 'Michael', 'last': 'Heym'}, 'id': 'Michael-Heym'},\n",
       " {'canonical': {'first': 'Alejandra I.', 'last': 'Huerta'},\n",
       "  'id': 'Alejandra I-Huerta'},\n",
       " {'canonical': {'first': 'Patrick A.', 'last': 'Jansen'},\n",
       "  'id': 'Patrick A-Jansen'},\n",
       " {'canonical': {'first': 'Daniel J.', 'last': 'Johnson'},\n",
       "  'id': 'Daniel J-Johnson'},\n",
       " {'canonical': {'first': 'Nikolai', 'last': 'Knapp'}, 'id': 'Nikolai-Knapp'},\n",
       " {'canonical': {'first': 'Kamil', 'last': 'Král'}, 'id': 'Kamil-Král'},\n",
       " {'canonical': {'first': 'Dunmei', 'last': 'Lin'}, 'id': 'Dunmei-Lin'},\n",
       " {'canonical': {'first': 'Yadvinder', 'last': 'Malhi'},\n",
       "  'id': 'Yadvinder-Malhi'},\n",
       " {'canonical': {'first': 'Sean M.', 'last': 'McMahon'},\n",
       "  'id': 'Sean M-McMahon'},\n",
       " {'canonical': {'first': 'Jonathan A.', 'last': 'Myers'},\n",
       "  'id': 'Jonathan A-Myers'},\n",
       " {'canonical': {'first': 'David', 'last': 'Orwig'}, 'id': 'David-Orwig'},\n",
       " {'canonical': {'first': 'Diego I.', 'last': 'Rodríguez‐Hernández'},\n",
       "  'id': 'Diego I-Rodríguez‐Hernández'},\n",
       " {'canonical': {'first': 'Sabrina E.', 'last': 'Russo'},\n",
       "  'id': 'Sabrina E-Russo'},\n",
       " {'canonical': {'first': 'Jessica', 'last': 'Shue'}, 'id': 'Jessica-Shue'},\n",
       " {'canonical': {'first': 'Xugao', 'last': 'Wang'}, 'id': 'Xugao-Wang'},\n",
       " {'canonical': {'first': 'Amy', 'last': 'Wolf'}, 'id': 'Amy-Wolf'},\n",
       " {'canonical': {'first': 'Tonghui', 'last': 'Yang'}, 'id': 'Tonghui-Yang'},\n",
       " {'canonical': {'first': 'Stuart J.', 'last': 'Davies'},\n",
       "  'id': 'Stuart J-Davies'},\n",
       " {'canonical': {'first': 'Kristina J.', 'last': 'Anderson‐Teixeira'},\n",
       "  'id': 'Kristina J-Anderson‐Teixeira'},\n",
       " {'canonical': {'first': 'Patrick', 'last': 'Lloyd‐Smith'},\n",
       "  'id': 'Patrick-Lloyd‐Smith'},\n",
       " {'canonical': {'first': 'Yifeng', 'last': 'Wang'}, 'id': 'Yifeng-Wang'},\n",
       " {'canonical': {'first': 'Robert G.', 'last': 'Way'}, 'id': 'Robert G-Way'},\n",
       " {'canonical': {'first': 'Jordan', 'last': 'Beer'}, 'id': 'Jordan-Beer'},\n",
       " {'canonical': {'first': 'Anika', 'last': 'Forget'}, 'id': 'Anika-Forget'},\n",
       " {'canonical': {'first': 'Rosamond', 'last': 'Tutton'},\n",
       "  'id': 'Rosamond-Tutton'},\n",
       " {'canonical': {'first': 'Meredith C.', 'last': 'Purcell'},\n",
       "  'id': 'Meredith C-Purcell'}]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resource not found. 10.1001/hyp.14465\n",
    "Resource not found. 10.1016/j.hydrol.2021.126793\n",
    "Resource not found. 10.20383/101.0275\n",
    "Resource not found. 10.22004/ag.econ.302498\n",
    "Resource not found. 10.4230/lipics.swat.2018.12\n",
    "Resource not found. 10.48550/arxiv.2204.05345\n",
    "Resource not found. 10.5683/sp2/zrgxq5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6560448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5194/tc-14-2795-2020-supplement\n",
      "10.5194/hess-24-2731-2020-supplement\n",
      "10.5194/hess-24-2141-2020-supplement\n",
      "10.1175/bams-d-18-0268.1\n",
      "10.5194/bg-17-361-2020-supplement\n",
      "10.1029/tr002i001p00002\n"
     ]
    }
   ],
   "source": [
    "def compute_hash(value: bytes) -> str:\n",
    "    checksum = crc32(value) & 0xFFFFFFFF\n",
    "    return f\"{checksum:08x}\"\n",
    "\n",
    "def generate_bibkey(title, author, year, bibkey_list):\n",
    "    author_list = [(item['family'] if 'family' in item else '') for item in author] + [(item['given'] if 'given' in item else '') for item in author]    \n",
    "    title_list = title.split()\n",
    "    count=1\n",
    "    while True:\n",
    "        bibkey = '-'.join(author_list[:count] + [str(year)] + title_list[:count])\n",
    "        if bibkey not in bibkey_list: return bibkey\n",
    "        count += 1\n",
    "\n",
    "with open('result.json', 'r') as openfile:     \n",
    "    json_object = json.load(openfile)\n",
    "\n",
    "# with open(abstract_file, 'r') as openfile:     \n",
    "#     abstract_dict = json.load(openfile)\n",
    "\n",
    "with_abs, without_abs = 0, 0\n",
    "\n",
    "for key, item in json_object.items():\n",
    "    if ('author' not in item) or (not item['title']):\n",
    "        print(key)\n",
    "\n",
    "# paper_dict = {}\n",
    "# json_object = {key: item for key, item in json_object.items() if ('author' in item) and (item['title'])} # drop papers without title or author \n",
    "# count = 0\n",
    "# count1 = 0\n",
    "# count2 = 0\n",
    "# no_month = 0 \n",
    "# for key, item in json_object.items():\n",
    "#     try:        \n",
    "#         year = item['published']['date-parts'][0][0]\n",
    "#         if len(item['published']['date-parts'][0])<2: \n",
    "#             no_month += 1\n",
    "#             print(key, year)\n",
    "\n",
    "#         count1 += 1\n",
    "#         journal = item['container-title'][0] if item['container-title'] else ' '\n",
    "#         count2 += 1\n",
    "#         volume = item['volume'] if 'volume' in item else ''\n",
    "#         issue = item['issue'] if 'issue' in item else ''\n",
    "#         pub_key = 'journal: '+journal+' volume: '+volume+' issue: '+issue\n",
    "#         if year not in paper_dict: paper_dict[year]={}\n",
    "#         if pub_key not in paper_dict[year]: paper_dict[year][pub_key]={}\n",
    "#         paper_dict[year][pub_key][key] = item     \n",
    "#     except:        \n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dd994bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom as md\n",
    "\n",
    "xml_fname = '''<?xml version='1.0' encoding='UTF-8'?>\n",
    "<collection id=\"G14\"><volume id=\"1\"><meta><booktitle>Environmental Health Insights, Volume 8</booktitle><publisher>SAGE Publications</publisher><address /><year>2014</year></meta><paper id=\"1\"><title>A Framework for Building Research Partnerships with First Nations Communities</title><author><first>Lalita</first><last>Bharadwaj</last></author><abstract>Solutions to complex health and environmental issues experienced by First Nations communities in Canada require the adoption of collaborative modes of research. The traditional “helicopter” approach to research applied in communities has led to disenchantment on the part of First Nations people and has impeded their willingness to participate in research. University researchers have tended to develop projects without community input and to adopt short term approaches to the entire process, perhaps a reflection of granting and publication cycles and other realities of academia. Researchers often enter communities, collect data without respect for local culture, and then exit, having had little or no community interaction or consideration of how results generated could benefit communities or lead to sustainable solutions. Community-based participatory research (CBPR) has emerged as an alternative to the helicopter approach and is promoted here as a method to research that will meet the objectives of both First Nations and research communities. CBPR is a collaborative approach that equitably involves all partners in the research process. Although the benefits of CBPR have been recognized by segments of the University research community, there exists a need for comprehensive changes in approaches to First Nations centered research, and additional guidance to researchers on how to establish respectful and productive partnerships with First Nations communities beyond a single funded research project. This article provides a brief overview of ethical guidelines developed for researchers planning studies involving Aboriginal people as well as the historical context and principles of CBPR. A framework for building research partnerships with First Nations communities that incorporates and builds upon the guidelines and principles of CBPR is then presented. The framework was based on 10 years’ experience working with First Nations communities in Saskatchewan. The framework for research partnership is composed of five phases. They are categorized as the pre-research, community consultation, community entry, research and research dissemination phases. These phases are cyclical, non-linear and interconnected. Elements of, and opportunities for, exploration, discussion, engagement, consultation, relationship building, partnership development, community involvement, and information sharing are key components of the five phases within the framework. The phases and elements within this proposed framework have been utilized to build and implement sustainable collaborative environmental health research projects with Saskatchewan First Nations communities.</abstract><url hash=\"0cb058e8\">G14-1001</url><pages>EHI.S10869</pages><doi>10.4137/ehi.s10869</doi><bibkey>Bharadwaj-2014-A</bibkey></paper></volume></collection>'''\n",
    "\n",
    "uglyxml = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><employees><employee><Name>Leonardo DiCaprio</Name></employee></employees>'\n",
    "\n",
    "# xml = xml.dom.minidom.parseString(xml_fname)\n",
    "xml = md.parse('G21.xml')\n",
    "\n",
    "xml_pretty_str = xml.toprettyxml(encoding=\"utf-8\").decode()\n",
    "# print(xml_pretty_str)\n",
    "\n",
    "with open(\"test.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(xml_pretty_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2987d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Morteza1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "string_check = re.compile(\"[@_!#$%^&*()<>?/\\|}{~:-]\")\n",
    "nltk.download(\"stopwords\")\n",
    "list_stopwords = list(stopwords.words(\"english\"))\n",
    "paper_url = \"https://api.semanticscholar.org/v1/paper/\"\n",
    "search_url = \"https://api.semanticscholar.org/graph/v1/paper/search?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "527b3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"+\".join(pub.split()).lower()                    \n",
    "payload = {'offset': 0, 'limit': 10,'query': query}\n",
    "\n",
    "PARAMS = urllib.parse.urlencode(payload, safe=':+')\n",
    "\n",
    "headers={'x-api-key':'LPkwK92ydta0i2EY5UB8fgnVhoLPZb72T3p3TCF1'}\n",
    "\n",
    "r = requests.get(url=search_url, params=PARAMS, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12968544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "paper_df = pd.read_csv('DOI_all.csv', header=0).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2213a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3390/SU13042415\n"
     ]
    }
   ],
   "source": [
    "# paper_df.drop_duplicates()\n",
    "for item in paper_df['doi']:\n",
    "    if '10.3390/SU13042415' in item: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16ff91c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         10.3390/agriculture11080739\n",
       "1                  10.1002/agj2.21056\n",
       "2            10.3390/hydrology8010001\n",
       "3             10.2489/jswc.2021.00099\n",
       "4       10.1080/07011784.2022.2032365\n",
       "                    ...              \n",
       "1023                              NaN\n",
       "1024                   10.1101/230722\n",
       "1025                              NaN\n",
       "1026        10.5194/essd-14-5139-2022\n",
       "1027    10.1016/j.jhydrol.2022.128711\n",
       "Name: doi, Length: 1028, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_df['doi'] =paper_df['doi'].str.lower().drop_duplicates()\n",
    "paper_df['doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638f9b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "None doi\n",
      "4\n",
      "None doi\n",
      "14\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "None doi\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "None doi\n",
      "63\n",
      "64\n",
      "None doi\n",
      "71\n",
      "81\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "91\n",
      "96\n",
      "97\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "paper_df = pd.read_csv('GWF_all.csv', header=0, encoding='ascii', encoding_errors='ignore').fillna(\"\")\n",
    "\n",
    "paper_list = [None] * len(paper_df)\n",
    "# doi_list = [None] * len(paper_df)\n",
    "# score = [None] * len(paper_df)\n",
    "title = [None] * len(paper_df)\n",
    "title_after = [None] * len(paper_df)\n",
    "similarity_score = [None] * len(paper_df)\n",
    "# query = [None] * len(paper_df)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(paper_df)):\n",
    "    publication = paper_df.loc[i][0]\n",
    "    pub = re.split(\"([Dd][Oo][Ii])\", publication)\n",
    "    if len(pub) < 2:\n",
    "        paper_list[i] = pub[0]\n",
    "        \n",
    "        pub = re.split('[,.\"]', pub[0])\n",
    "        pub = pub[np.argmax(np.array([len(item) for item in pub if all(word not in item for word in ['Proceedings', 'Conference', 'Journal'])]))]\n",
    "\n",
    "        pub = pub.replace(\"'\", \"\").replace('\"', '').replace(':', '').replace('-', ' ')\n",
    "        title[i] = pub\n",
    "        pub = pub.split()\n",
    "\n",
    "        query_len = len(pub)\n",
    "        \n",
    "        query = \"+\".join(pub).lower()                    \n",
    "        payload = {'offset': 0, 'limit': 10,'query': query,}\n",
    "\n",
    "        PARAMS = urllib.parse.urlencode(payload, safe=':+')\n",
    "        headers={'x-api-key':'LPkwK92ydta0i2EY5UB8fgnVhoLPZb72T3p3TCF1'}        \n",
    "        r = requests.get(url=search_url, params=PARAMS, headers=headers)\n",
    "        if r.status_code!=200: print(\"r: \" + str(r.content))\n",
    "        \n",
    "        try:            \n",
    "            data = r.json()            \n",
    "            score = []\n",
    "            for paper in data[\"data\"]:\n",
    "                temp = difflib.SequenceMatcher(None, paper[\"title\"], ' '.join(pub))\n",
    "                a = temp.get_matching_blocks()\n",
    "                score.append(temp.ratio())\n",
    "\n",
    "            url2 = paper_url + data[\"data\"][score.index(max(score))][\"paperId\"]\n",
    "            similarity_score[i] = max(score)\n",
    "            title_after[i] = data[\"data\"][score.index(max(score))][\"title\"]\n",
    "\n",
    "            r2 = requests.get(url=url2, headers=headers)\n",
    "            if r2.status_code!=200: print(\"r2: \" + str(r2.content))\n",
    "            data2 = r2.json()\n",
    "            paper_doi = data2[\"doi\"]\n",
    "        except:        \n",
    "            paper_doi = None\n",
    "            similarity_score[i] = 'exception'\n",
    "            title_after[i] = 'exception'\n",
    "            print(\"None doi\")\n",
    "        \n",
    "        # print('paper_list:', paper_list[i])        \n",
    "        # print('title:', title[i])\n",
    "        # print('title_after:', title_after[i])\n",
    "        print(i)\n",
    "\n",
    "    if i==100: break    \n",
    "\n",
    "result_df['paper'] = paper_list\n",
    "result_df['title'] = title\n",
    "result_df['title_after'] = title_after\n",
    "result_df['similarity_score'] = similarity_score\n",
    "\n",
    "result_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab82c99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Lookinghorse'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub = re.split('[,.\"]', 'Noronha, N., Avarino, A., Balakumar, S., Toy, K., Smith, S., Wekerle, C., Martin-Hill, D., Lookinghorse, M., Drossos, A., Niec, A., Jacobs, B., Thomasen, K., & Lokker, C. â€œMental Health Mobile Applications Developed for Indigenous Communities in Canada: A Scoping Reviewâ€ Canadian Journal of Community Mental Health 41 (1): 1 - 5, 2022. https://')\n",
    "a = pub\n",
    "b = np.array([len(item) for item in pub if all(word not in item for word in ['Proceedings', 'Conference', 'Journal'])])\n",
    "pub = pub[np.argmax(np.array([len(item) for item in pub if all(word not in item for word in ['Proceedings', 'Conference', 'Journal'])]))]\n",
    "pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa120b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "publication = \"McCarter CPR, Wilkinson SL, Moore PA, Waddington JM. 2021. Ecohydrological trade-offs from multiple peatland disturbances: The interactive effects of drainage, harvesting, restoration and wildfire in a Southern Ontario bog. Journal of Hydrologyâ€¯601: 126793, \"\n",
    "\n",
    "pub = re.split(\"[,.:]\", publication)\n",
    "pub = pub[np.argmax(np.array([len(item) for item in pub]))]\n",
    "\n",
    "pub = re.sub(\"([A-Z][.][,])\", \"\", pub)\n",
    "pub = re.sub(\"([A-Z][.][A-Z][.])\", \"\", pub).replace(\".\", \"\").replace('\"', \"\")\n",
    "pub = re.sub(\"([A-Z][.])\", \"\", pub).replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "pub = re.sub(\"([(][0-9]{4}[)])\", \"\", pub)\n",
    "\n",
    "pub = [i for item in pub.split(\",\") for i in item.strip().split()]\n",
    "pub = [item for item in pub if item.lower() not in list_stopwords and len(item) > 2]\n",
    "\n",
    "pub = [\n",
    "    item\n",
    "    for item in pub\n",
    "    if (string_check.search(item) == None) and not bool(re.search(r\"\\d\", item))\n",
    "]\n",
    "\n",
    "i = 6\n",
    "while True:\n",
    "    try:\n",
    "        query = \"+\".join(pub[:i]).lower()\n",
    "        break\n",
    "    except:\n",
    "        i -= 1\n",
    "            \n",
    "payload = {'offset': 0, 'limit': 50,'query': query}\n",
    "\n",
    "PARAMS = urllib.parse.urlencode(payload, safe=':+')\n",
    "r = requests.get(url=search_url, params=PARAMS)\n",
    "if r.status_code!=200: print(\"r: \" + str(r.content))\n",
    "\n",
    "try:\n",
    "    data = r.json()\n",
    "    if data[\"total\"] > 1:\n",
    "        score = []\n",
    "        for paper in data[\"data\"]:\n",
    "            temp = difflib.SequenceMatcher(None, paper[\"title\"], ' '.join(pub))\n",
    "            a = temp.get_matching_blocks()\n",
    "            score.append(temp.ratio())\n",
    "\n",
    "        url2 = paper_url + data[\"data\"][score.index(max(score))][\"paperId\"]\n",
    "        similarity_score = max(score)\n",
    "        title = data[\"data\"][score.index(max(score))][\"title\"]\n",
    "    else:\n",
    "        url2 = paper_url + data[\"data\"][0][\"paperId\"]\n",
    "        similarity_score = difflib.SequenceMatcher(None, data[\"data\"][0][\"title\"], ' '.join(pub)).ratio() + 5\n",
    "        title = data[\"data\"][0][\"title\"]\n",
    "\n",
    "    r2 = requests.get(url=url2)\n",
    "    if r2.status_code!=200: print(\"r2: \" + str(r2.content))\n",
    "    data2 = r2.json()\n",
    "    paper_doi = data2[\"doi\"]\n",
    "except:        \n",
    "    paper_doi = None\n",
    "    similarity_score = 'exception'\n",
    "    title = 'exception'\n",
    "    print(\"None doi\")\n",
    "\n",
    "return paper_doi, similarity_score, title, query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1c16427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as et\n",
    "from zlib import crc32 \n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9733b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = pd.DataFrame(columns=[\"title\", \"author\", \"abstract\", \"url\", \"pages\", \"doi\", \"bibkey\"])\n",
    "tree = et.parse('G17.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for paper in root.iter('paper'):    \n",
    "    paper_dict = {}\n",
    "    paper_dict[\"title\"] = paper.find(\"title\").text\n",
    "    paper_dict[\"author\"] = \"\"\n",
    "    for author in paper.findall(\"author\"):\n",
    "        paper_dict[\"author\"] = paper_dict[\"author\"] + author.find(\"first\").text + \" \" + author.find(\"last\").text + \", \"\n",
    "    paper_dict[\"abstract\"] = paper.find(\"abstract\").text\n",
    "    paper_dict[\"url\"] = paper.find(\"url\").text       \n",
    "    paper_dict[\"pages\"] = paper.find(\"pages\").text if paper.find(\"pages\") is not None else \"\"\n",
    "    paper_dict[\"doi\"] = paper.find(\"doi\").text\n",
    "    paper_dict[\"bibkey\"] = paper.find(\"bibkey\").text\n",
    "\n",
    "    sheet = pd.concat([sheet, pd.DataFrame([paper_dict.values()], columns=sheet.columns)], ignore_index=True)\n",
    "\n",
    "sheet.to_csv(\"G17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ffc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = et.parse('G18.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for paper in root.iter(\"paper\"):\n",
    "    abstract = paper.find(\"abstract\").text\n",
    "    paper.find(\"abstract\").text = html.unescape(abstract)\n",
    "    print(abstract)\n",
    "\n",
    "with open (\"test.xml\", \"wb\") as files :\n",
    "    tree.write(files, encoding='UTF-8', xml_declaration=True, method='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e221229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = et.parse('G18.xml')\n",
    "for element in tree.iter():\n",
    "    text = element.text\n",
    "    if text: element.text = html.unescape(text)\n",
    "\n",
    "with open (\"test.xml\", \"wb\") as files :\n",
    "    tree.write(files, encoding='UTF-8', xml_declaration=True, method='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "38e098fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aas\n"
     ]
    }
   ],
   "source": [
    "if not text: print(\"aas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8da340db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%682m\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "print(html.unescape('&#x0025;682m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3131d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in author:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241cad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'r') as openfile:     \n",
    "    json_object = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34253fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dict = {}\n",
    "for idx, key in enumerate(json_object):\n",
    "    if json_object[key]['year'] not in paper_dict: paper_dict[json_object[key]['year']]={}\n",
    "    paper_dict[json_object[key]['year']][key] = json_object[key]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac2039fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journals_list = []\n",
    "\n",
    "for k, paper in paper_dict.items():\n",
    "    for key, item in paper.items():#['165c581c0ed8f39eca705f61e6b05dc353fce909']['journal']        \n",
    "        # try: journals_list.append([item['journal']['name'], item['externalIds']['DOI'], item['year']])\n",
    "        break\n",
    "        try: journals_list.append(item['journal']['name'])\n",
    "        except: pass\n",
    "\n",
    "journals_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "654dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "publication = \"Ronnquist, A.L., and Westbrook, C.J. 2021. Beaver dams: how structure, flow state, and landscape setting regulate water storage and release. Science of the Total Environment, 785: 147333, doi: https://doi.org/10.1016/j.scitotenv.2021.147333.Â \"\n",
    "pub = re.split(\"([Dd][Oo][Ii])\", publication)\n",
    "doi = (pub[-1].split()[0].replace('.org/', '') if pub[-1].startswith('.org/') else pub[2].replace(':', '').split()[0]) if len(pub) >= 3 else None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "450e80ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,1016/j.dendro.2017.01.003.\n",
      "10.5194/hess-2017-531.\n",
      "10.1016/j.jhy\n",
      "10.2134/jed2017.10.0398.\n"
     ]
    }
   ],
   "source": [
    "paper_list = [None] * len(paper_df)\n",
    "doi_list = [None] * len(paper_df)\n",
    "for i in range(len(paper_df)):\n",
    "    publication = paper_df.loc[i][0]\n",
    "    pub = re.split(\"([Dd][Oo][Ii])\", publication)\n",
    "    doi_list[i] = (pub[2].split()[0].replace('.org/', '') if pub[2].startswith('.org/') else pub[2].replace(':', '').split()[0]) if len(pub) == 3 else None    \n",
    "    \n",
    "    if doi_list[i]: \n",
    "        if requests.get(url='https://api.semanticscholar.org/v1/paper/' + doi_list[i]).status_code != 200:\n",
    "            doi_list[i] = doi_list[i][:-1] if requests.get(url='https://api.semanticscholar.org/v1/paper/' + doi_list[i][:-1]).status_code == 200 else None\n",
    "                \n",
    "    paper_list[i] = pub[0]\n",
    "\n",
    "paper_df['doi'] = doi_list\n",
    "paper_df['paper'] = paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "519333f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url =('https://api.semanticscholar.org/graph/v1/paper/10.5194/hess-2018-56?fields=paperId,'\n",
    "    'externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,openAccessPdf,'\n",
    "    'fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,journal,citationStyles,authors,citations,references'))\n",
    "r.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e43f009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'e7ca1d33678d2efe8cf215a350e8830842d4e2f9',\n",
       " 'externalIds': {'DOI': '10.5194/hess-2018-56', 'CorpusId': 201632713},\n",
       " 'url': 'https://www.semanticscholar.org/paper/e7ca1d33678d2efe8cf215a350e8830842d4e2f9',\n",
       " 'title': 'Combined Impacts of ENSO and MJO on the 2015 Growing 1 Season Drought on the Canadian Prairies 2',\n",
       " 'abstract': '1Global Institute for Water Security, University of Saskatchewan, Saskatoon, Saskatchewan, Canada S7N3H5 4 2Institute of Space and Atmospheric Studies, University of Saskatchewan, Saskatoon, Saskatchewan, Canada 5 3National Hydrology Research Center, Environment and Climate Change Canada, Saskatoon, SK, Canada 6 Correspondence to: Dr. Yanping Li (yanping.li@usask.ca); Dr. Zhenhua Li (zhenhua.li@usask.ca) 7 Abstract 8',\n",
       " 'venue': '',\n",
       " 'year': 2018,\n",
       " 'referenceCount': 38,\n",
       " 'citationCount': 1,\n",
       " 'influentialCitationCount': 0,\n",
       " 'isOpenAccess': True,\n",
       " 'openAccessPdf': {'url': 'https://www.hydrol-earth-syst-sci.net/22/5057/2018/hess-22-5057-2018.pdf',\n",
       "  'status': 'GREEN'},\n",
       " 'fieldsOfStudy': None,\n",
       " 's2FieldsOfStudy': [{'category': 'Environmental Science',\n",
       "   'source': 's2-fos-model'},\n",
       "  {'category': 'Agricultural And Food Sciences', 'source': 's2-fos-model'}],\n",
       " 'publicationTypes': None,\n",
       " 'publicationDate': None,\n",
       " 'journal': None,\n",
       " 'citationStyles': {'bibtex': '@None{Li2018CombinedIO,\\n author = {Zhenhua Li and Yanping Li and B. Bonsal and A. Manson and L. Scaff},\\n title = {Combined Impacts of ENSO and MJO on the 2015 Growing 1 Season Drought on the Canadian Prairies 2},\\n year = {2018}\\n}\\n'},\n",
       " 'authors': [{'authorId': '121544552', 'name': 'Zhenhua Li'},\n",
       "  {'authorId': '2111183095', 'name': 'Yanping Li'},\n",
       "  {'authorId': '6062435', 'name': 'B. Bonsal'},\n",
       "  {'authorId': '49147159', 'name': 'A. Manson'},\n",
       "  {'authorId': '104655836', 'name': 'L. Scaff'}],\n",
       " 'citations': [{'paperId': '22f416571aa25977e231001ff0641861dbd00345',\n",
       "   'title': 'Evaluation of convection-permitting WRF CONUS simulation on the relationship between soil moisture and heatwaves'}],\n",
       " 'references': [{'paperId': 'f95b245d5266ba81adf1400b9b9d73ade1d09612',\n",
       "   'title': 'The NCEP/NCAR 40-Year Reanalysis Project'},\n",
       "  {'paperId': 'abee35dd276902a457f0a2d0fb37d92210fe8b8f',\n",
       "   'title': 'Circulation characteristics of persistent cold spells in central–eastern North America'},\n",
       "  {'paperId': None, 'title': 'Circulation Characteristics of Persistent'},\n",
       "  {'paperId': '482fe371f9a5e68884757d40f4c424767953dea4',\n",
       "   'title': 'On the role of anomalous ocean surface temperatures for promoting the record Madden‐Julian Oscillation in March 2015'},\n",
       "  {'paperId': '51485c014684e07737daaa6fd47d7645b8986eb6',\n",
       "   'title': 'Tropical Oceanic Rainfall and Sea Surface Temperature Structure: Parsing Causation from Correlation in the MJO'},\n",
       "  {'paperId': '8d81e9f4132e7da1f87141cf907643467cae38e0',\n",
       "   'title': 'Pacific sea surface temperature and the winter of 2014'},\n",
       "  {'paperId': '821a6b731f61e7d2aee8a185a6a2c83d83e98008',\n",
       "   'title': 'Compounding effects of warm sea surface temperature and reduced sea ice on the extreme circulation over the extratropical North Pacific and North America during the 2013–2014 boreal winter'},\n",
       "  {'paperId': 'a4afb1b63d92781bcbd5be26863ed2f2b6b46602',\n",
       "   'title': 'Extended Reconstructed Sea Surface Temperature Version 4 (ERSST.v4). Part I: Upgrades and Intercomparisons'},\n",
       "  {'paperId': '70a57d7ee4a2c59b8abc9a835e86a99ade92bbec',\n",
       "   'title': 'Probable causes of the abnormal ridge accompanying the 2013–2014 California drought: ENSO precursor and anthropogenic warming footprint'},\n",
       "  {'paperId': None,\n",
       "   'title': '2016: The June 2013 Alberta Catastrophic Flooding Event: Part 1 – Large scale features'},\n",
       "  {'paperId': '2ca911e9aab3aa70ef92048fe209e98ea79fd8e8',\n",
       "   'title': 'Observed connection between stratospheric sudden warmings and the Madden‐Julian Oscillation'},\n",
       "  {'paperId': '7cfa7bde44e9ae151fd972b7cf57245b7aef7cce',\n",
       "   'title': 'Excitation of rainfall over the Tropical Western Pacific'},\n",
       "  {'paperId': 'ca009cb169c2c09cb0ac313bc905d1ca2555cea0',\n",
       "   'title': 'Atmospheric and Oceanic Variability Associated with Growing Season Droughts and Pluvials on the Canadian Prairies'},\n",
       "  {'paperId': '201b76815c9efc63b534b3c20c23423b72b6ec4f',\n",
       "   'title': 'The ERA‐Interim reanalysis: configuration and performance of the data assimilation system'},\n",
       "  {'paperId': '05e60ab3a03a3b5f6acb547945aa2055f44f88cb',\n",
       "   'title': 'Intraseasonal interaction between the Madden–Julian Oscillation and the North Atlantic Oscillation'},\n",
       "  {'paperId': 'be90cd638bea8323b0b8c1e24542e4c27a6f046a',\n",
       "   'title': 'Historical comparison of the 2001/2002 drought in the Canadian Prairies'},\n",
       "  {'paperId': '25a3ad14e24abb87440c52b29d03b4ec6b94ed38',\n",
       "   'title': 'Atmospheric circulation comparisons between the 2001 and 2002 and the 1961 and 1988 Canadian prairie droughts'},\n",
       "  {'paperId': '5b29e5e55add850a6968449314dfa9ea820141a9',\n",
       "   'title': 'An All-Season Real-Time Multivariate MJO Index: Development of an Index for Monitoring and Prediction'},\n",
       "  {'paperId': '683bcac11e60a011d2be45a3d5b860af95e3fefd',\n",
       "   'title': 'Summer Drought Patterns in Canada and the Relationship toGlobal Sea Surface Temperatures'},\n",
       "  {'paperId': '7e47f7e43ee5cde587f7ee051c3701c25e95d05c',\n",
       "   'title': 'Moisture Transport and Other Hydrometeorological Features Associated with the Severe 2000/01 Drought over the Western and Central Canadian Prairies'},\n",
       "  {'paperId': '44f141a67264176a5b2c23c7d44baec008d0b02e',\n",
       "   'title': 'Northern Winter Stationary Waves: Theory and Modeling'},\n",
       "  {'paperId': 'f26cfc2d0a51f4d16a9b1d9e1be7a6e11e0f0d2a',\n",
       "   'title': 'Influences of the Madden Julian Oscillations on Temperature and Precipitation in North America during ENSO-Neutral and Weak ENSO Winters'},\n",
       "  {'paperId': 'cdbdbdbf9119e8a473d8d5c7552eae9b227d1072',\n",
       "   'title': 'Impacts of low frequency variability modes on Canadian winter temperature'},\n",
       "  {'paperId': '2896e7f8333c2c88d0ea702503bbb250f063db8f',\n",
       "   'title': 'Global patterns of ENSO‐induced precipitation'},\n",
       "  {'paperId': '8a838fc4bb5ee0ff83d0233669b0e078c5654ba9',\n",
       "   'title': 'Teleconnections between El Niño and La Niña events and summer extended dry spells on the Canadian Prairies'},\n",
       "  {'paperId': '3271e3b3773698cb83031611ea1d4c6ee0bd0205',\n",
       "   'title': 'Interannual Variation of the Madden–Julian Oscillation during Austral Summer'},\n",
       "  {'paperId': '1da0edb57183551b5f304e3f4c26564244e70e3b',\n",
       "   'title': 'Canadian Prairie growing season precipitation variability and associated atmospheric circulation'},\n",
       "  {'paperId': '04254941cef1e9379ce4411bdd6e00b7061920e8',\n",
       "   'title': 'Canadian Precipitation Patterns Associated with the Southern Oscillation'},\n",
       "  {'paperId': 'e6b3461e00f8dd937aaa9b05cbb1245139bc9b81',\n",
       "   'title': 'Global Precipitation: A 17-Year Monthly Analysis Based on Gauge Observations, Satellite Estimates, and Numerical Model Outputs'},\n",
       "  {'paperId': '080d3b35913be09bcb20023abcc5ecdbc0844464',\n",
       "   'title': 'Teleconnections between North Pacific SST anomalies and growing season extended dry spells on the canadian prairies'},\n",
       "  {'paperId': '747ef4fe59b121290d141c27065bc1c71742cca7',\n",
       "   'title': 'Rossby Wave Propagation on a Realistic Longitudinally Varying Flow'},\n",
       "  {'paperId': None,\n",
       "   'title': 'Monitoring ENSO in COADS with a seasonally adjusted principal'},\n",
       "  {'paperId': 'd07c2f9694c31978aa1073a7022e9f210d564917',\n",
       "   'title': 'Examples of the Horizontal Propagation of Quasi-stationary Waves'},\n",
       "  {'paperId': '1165a5b7d6f1fa4825c8a2f82728f7ef8ac0aed9',\n",
       "   'title': 'The Southern Oscillation in Surface Circulation and Climate over the Tropical Atlantic, Eastern Pacific, and Indian Oceans as Captured by Cluster Analysis'},\n",
       "  {'paperId': '2c99a3c9d57403e46b7d47f494c8de8d22915f0f',\n",
       "   'title': 'Barotropic Wave Propagation and Instability, and Atmospheric Teleconnection Patterns.'},\n",
       "  {'paperId': '45f31ea7d447f462bc16f6cd19227380dd3bd7e6',\n",
       "   'title': 'Nature and possible causes of droughts on the canadian prairies‐case studies'},\n",
       "  {'paperId': 'c6d409653e94acd9c8cd5320cb81d09e4279db96',\n",
       "   'title': 'The Steady Linear Response of a Spherical Atmosphere to Thermal and Orographic Forcing'},\n",
       "  {'paperId': '77802efc29b4658f188aa01a65b22d2ca09c4ab3',\n",
       "   'title': 'Detection of a 40–50 Day Oscillation in the Zonal Wind in the Tropical Pacific'}]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb8655d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r.json()\n",
    "type(data)\n",
    "json_object = json.dumps(data, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc8baabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.json', 'r') as openfile:\n",
    " \n",
    "    # Reading from json file\n",
    "    json_object = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "860fa103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb7abe5b\n"
     ]
    }
   ],
   "source": [
    "from zlib import crc32 \n",
    "\n",
    "def compute_hash(value: bytes) -> str:\n",
    "    checksum = crc32(value) & 0xFFFFFFFF\n",
    "    return f\"{checksum:08x}\"\n",
    "\n",
    "print(compute_hash(str.encode(\"https://aclanthology.org/J19-1001.pdf\")))\n",
    "# \"J19-1001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d6057149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "dict_file = [{'canonical' : {'first': 'Maricor', 'last': 'Arlos'}, 'id':'654'}]\n",
    "\n",
    "\n",
    "# dict_file = dict(\n",
    "#     A = 'a',\n",
    "#     B = dict(\n",
    "#         C = 'c',\n",
    "#         D = 'd',\n",
    "#         E = 'f',\n",
    "#     )\n",
    "# )\n",
    "\n",
    "with open(r'store_file.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_file, file, default_flow_style=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "24bf29e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_file = []\n",
    "for key in json_object:\n",
    "    for item in json_object[key]['authors']:\n",
    "        element = {'canonical' : {'first': item['name'].split(' ',1)[0], 'last': item['name'].split(' ',1)[-1]}, 'id':item['name'].replace('.', '').replace(' ', '-')}\n",
    "        if element not in dict_file: dict_file.append(element)\n",
    "\n",
    "len(dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "95f5e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url =(\"https://api.crossref.org/works/10.1007/s10664-020-09921-9\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "39e0339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID-correspondence: a measure for detecting evolutionary coupling']"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['message']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0a660ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192771084337349"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "difflib.SequenceMatcher(None,  'Facilitating Asynchronous Collaboration in Scientific Workflow Composition Using Provenance',\"facilitating collaborative scientific workflow composition using Provenance\").ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "10323be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Feature Transformation for Improved Software Bug Detection Models\"'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub = re.split(\"([,.:])\", 'Cynthia ST, Roy B and *Mondal D, \"Feature Transformation for Improved Software Bug Detection Models\", ACM 15th Innovation in Software Engineering Conference (ISEC 2022), Article 16, pp. 1-10, DA-IICT Gandhinagar, February 2022. ')\n",
    "pub = pub[np.argmax(np.array([len(item) for item in pub]))]\n",
    "pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "35d80ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Match(a=0, b=0, size=2), Match(a=2, b=9, size=1), Match(a=5, b=12, size=9), Match(a=14, b=25, size=6), Match(a=20, b=31, size=0)]\n",
      "Similarity Score:  0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "string1 = \"Carbon dioxide efflux and ecosystem metabolism of small forest lakes\"\n",
    "string2 = \"Ecosystem influences carbon water dynamics two temperate forage crops\"\n",
    "\n",
    "temp = difflib.SequenceMatcher(None,string1 ,string2)\n",
    "\n",
    "print(temp.get_matching_blocks())\n",
    "print('Similarity Score: ',temp.ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dc778e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "Resource not found.\n",
      "exception\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "Resource not found.\n",
      "exception\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "Resource not found.\n",
      "exception\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n"
     ]
    }
   ],
   "source": [
    "data_url =('    {DOI}')\n",
    "\n",
    "all_data_dict = {}\n",
    "\n",
    "paper_df = pd.read_csv('DOI_all.csv', header=0).fillna(\"\")['doi']\n",
    "\n",
    "for i in range(len(paper_df)):\n",
    "    print(i)\n",
    "    try:\n",
    "        url = data_url.replace(\"{DOI}\", paper_df[i])\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            print(r.text)\n",
    "\n",
    "        data = r.json()\n",
    "        all_data_dict[data['message']['DOI']] = data['message']\n",
    "    except: \n",
    "        print('exception')\n",
    "        pass    \n",
    "    \n",
    "    if i==100:\n",
    "        json_object = json.dumps(all_data_dict, indent=4)\n",
    "        with open('result.json', \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "json_object = json.dumps(all_data_dict, indent=4)\n",
    "with open('result.json', \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "801bff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objecta = {key: itema for key, itema in json_object.items() if 'author' in itema}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8798326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37dcd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'r') as openfile:     \n",
    "    json_object = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ad7c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dict = {}\n",
    "json_object = {key: item for key, item in json_object.items() if ('author' in item) and (item['title'])} # drop papers without title or author \n",
    "for key, item in json_object.items():\n",
    "    try:\n",
    "        year = item['published']['date-parts'][0][0]\n",
    "        journal = item['container-title'][0]\n",
    "        volume = item['volume'] if 'volume' in item else ''\n",
    "        issue = item['issue'] if 'issue' in item else ''\n",
    "        pub_key = 'journal: '+journal+' volume: '+volume+' issue: '+issue\n",
    "        if year not in paper_dict: paper_dict[year]={}\n",
    "        if pub_key not in paper_dict[year]: paper_dict[year][pub_key]={}\n",
    "        paper_dict[year][pub_key][key] = item     \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21feb49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibkey Liu-2021-Phosphorus\n",
      "bibkey Clark-2021-The\n",
      "bibkey Bhattacharjee-2022-Supporting\n",
      "bibkey Huang-2022-A\n",
      "bibkey Liu-2019-Impacts\n",
      "bibkey Lindenschmidt-2019-A\n",
      "bibkey Robinne-2019-A\n"
     ]
    }
   ],
   "source": [
    "def compute_hash(value: bytes) -> str:\n",
    "    checksum = crc32(value) & 0xFFFFFFFF\n",
    "    return f\"{checksum:08x}\"\n",
    "\n",
    "def generate_bibkey(title, author, year, bibkey_list):\n",
    "    author_list = [(item['family'] if 'family' in item else '') for item in author] + [(item['given'] if 'given' in item else '') for item in author]    \n",
    "    title_list = title.split()\n",
    "    count=1\n",
    "    while True:\n",
    "        bibkey = '-'.join(author_list[:count] + [str(year)] + title_list[:count])\n",
    "        if bibkey not in bibkey_list: return bibkey\n",
    "        count += 1\n",
    "        print('bibkey', bibkey)\n",
    "\n",
    "for year, year_dict in paper_dict.items():\n",
    "    tag_collection = et.Element(\"collection\")\n",
    "    tag_collection.set('id', \"G\"+str(year)[-2:])\n",
    "\n",
    "    bibkey_list = []\n",
    "\n",
    "    for index, (volume, item) in enumerate(year_dict.items()):\n",
    "        tag_vol = et.Element(\"volume\")\n",
    "        tag_collection.append(tag_vol)\n",
    "        tag_vol.set('id', str(index+1))\n",
    "\n",
    "        first_item = list(item.items())[0][1]\n",
    "\n",
    "        tag_meta = et.Element(\"meta\")\n",
    "        tag_vol.append(tag_meta)    \n",
    "        tag_subelement = et.SubElement(tag_meta, \"booktitle\")\n",
    "        tag_subelement.text = first_item['container-title'][0]+((', Volume '+first_item['volume']) if 'volume' in first_item else '')+((', Issue '+first_item['issue']) if 'issue' in first_item else '')\n",
    "        tag_subelement = et.SubElement(tag_meta, \"publisher\")\n",
    "        tag_subelement.text = first_item['publisher']\n",
    "        tag_subelement = et.SubElement(tag_meta, \"address\")\n",
    "        tag_subelement.text = \"\"\n",
    "        tag_subelement = et.SubElement(tag_meta, \"year\")\n",
    "        tag_subelement.text = str(year)\n",
    "\n",
    "        # tag_frontmatter = et.Element(\"frontmatter\")\n",
    "        # tag_vol.append(tag_frontmatter)\n",
    "        # tag_subelement = et.SubElement(tag_frontmatter, \"url\")\n",
    "        # tag_subelement.text = str(year)+\".GWF-1.0\"   \n",
    "        # tag_subelement.set('hash', compute_hash(str.encode(tag_subelement.text)))\n",
    "        # tag_subelement = et.SubElement(tag_frontmatter, \"bibkey\")\n",
    "        # tag_subelement.text = \"G\"+str(year)[-2:] +'-'+ str(year) + '-water'        \n",
    "\n",
    "        tmep_dict = year_dict[volume]\n",
    "        for idx, (key, tmep_dict_item) in enumerate(tmep_dict.items()):\n",
    "            tag_paper = et.Element(\"paper\")\n",
    "            tag_vol.append(tag_paper)\n",
    "            tag_paper.set('id', str(idx+1))\n",
    "            tag_subelement = et.SubElement(tag_paper, \"title\")\n",
    "            tag_subelement.text = tmep_dict_item['title'][0]\n",
    "            for author in tmep_dict_item['author']:\n",
    "                tag_subelement = et.SubElement(tag_paper, \"author\")\n",
    "                tag_subsubelement = et.SubElement(tag_subelement, \"first\")\n",
    "                tag_subsubelement.text = (author['given'] if 'given' in author else '')\n",
    "                tag_subsubelement = et.SubElement(tag_subelement, \"last\")\n",
    "                tag_subsubelement.text = (author['family'] if 'family' in author else '')\n",
    "\n",
    "            tag_subelement = et.SubElement(tag_paper, \"abstract\")\n",
    "            tag_subelement.text = tmep_dict_item['abstract'] if 'abstract' in tmep_dict_item else ''\n",
    "            tag_subelement = et.SubElement(tag_paper, \"url\")\n",
    "            tag_subelement.text =  f'G{str(year)[-2:]}-{index+1}{(idx+1):03}'\n",
    "            tag_subelement.set('hash', compute_hash(str.encode(tag_subelement.text)))\n",
    "            \n",
    "            if 'page' in tmep_dict_item:\n",
    "                tag_subelement = et.SubElement(tag_paper, \"pages\")\n",
    "                tag_subelement.text = tmep_dict_item['page']\n",
    "\n",
    "            tag_subelement = et.SubElement(tag_paper, \"doi\")\n",
    "            tag_subelement.text = tmep_dict_item['DOI']\n",
    "            tag_subelement = et.SubElement(tag_paper, \"bibkey\")\n",
    "            bibkey = generate_bibkey(tmep_dict_item['title'][0], tmep_dict_item['author'], year, bibkey_list)            \n",
    "            tag_subelement.text = bibkey\n",
    "            bibkey_list.append(bibkey)\n",
    "\n",
    "    # tree = et.ElementTree(tag_collection)    \n",
    "    # with open (\"G\"+str(year)[-2:]+\".xml\", \"wb\") as files :\n",
    "    #     tree.write(files, encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff33b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_file = []\n",
    "for key in json_object:\n",
    "    for item in json_object[key]['author']:\n",
    "        if ('given' in item) and ('family' in item): element = {'canonical' : {'first': (item['given'] if 'given' in item else ''), 'last': (item['family'] if 'family' in item else '')}, 'id':(item['given'] if 'given' in item else '').replace('.', '')+'-'+(item['family'] if 'family' in item else '').replace('.', '')}\n",
    "        if element not in dict_file: dict_file.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4597e924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ''.replace('.','')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a66ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not complete   2022 {'name': 'Six Nations Youth Mental Wellness Advisory Committee', 'sequence': 'additional', 'affiliation': []} ['Indigenous youth mental wellness and the adaptation of the JoyPopTM app (Preprint)']\n",
      "- not complete   2018 {'name': 'WCRP Global Sea Level Budget Group', 'sequence': 'first', 'affiliation': []} ['Global sea-level budget 1993–present']\n",
      "- not complete   2019 {'name': '1 Department of Computer Science, University of Saskatchewan, 110 Science Place, Saskatoon, SK, S7N 5C9, Canada', 'sequence': 'first', 'affiliation': []} ['On theoretical upper limits for valid timesteps of implicit ODE methods']\n",
      "- not complete   2019 {'name': '2 Department of Mathematics and Statistics, University of Saskatchewan, 106 Wiggins Road, Saskatoon, SK, S7N 5E6, Canada', 'sequence': 'additional', 'affiliation': []} ['On theoretical upper limits for valid timesteps of implicit ODE methods']\n",
      "- not complete   2019 {'name': 'Global Institute for Water Security, University of Saskatchewan, 11 Innovation Blvd., Saskatoon, Saskatchewan S7N 3H5, Canada', 'sequence': 'first', 'affiliation': []} ['A Stochastic Hydraulic Modelling Approach to Determining the Probable Maximum Staging of Ice-Jam Floods']\n",
      "- not complete   2019 {'name': 'Global Institute for Water Security, University of Saskatchewan, 11 Innovation Blvd., Saskatoon, Saskatchewan S7N 3H5, Canada', 'sequence': 'additional', 'affiliation': []} ['A Stochastic Hydraulic Modelling Approach to Determining the Probable Maximum Staging of Ice-Jam Floods']\n",
      "- not complete   2020 {'family': 'Lu', 'sequence': 'first', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2020 {'family': 'Tang', 'sequence': 'additional', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2020 {'family': 'Wang', 'sequence': 'additional', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2020 {'family': 'Liu', 'sequence': 'additional', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2020 {'family': 'Wei', 'sequence': 'additional', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2020 {'family': 'Zhang', 'sequence': 'additional', 'affiliation': []} ['The Development of a Two-Step Merging and Downscaling Method for Satellite Precipitation Products&#x0D;']\n",
      "- not complete   2019 {'family': 'Anonymous', 'sequence': 'first', 'affiliation': []} ['Review of “Role of sublimation and riming on the precipitation distribution in the Kananaskis Valley, Alberta, Canada”, by Poirier et al.']\n"
     ]
    }
   ],
   "source": [
    "dict_file = []\n",
    "json_object = {key: item for key, item in json_object.items() if ('author' in item) and (item['title'])} # drop papers without title or author \n",
    "for key in json_object:\n",
    "    for item in json_object[key]['author']:\n",
    "        if ('given' in item) and ('family' in item): dict_file.append((item['given'], item['family']))\n",
    "        else: print(\"- not complete  \", json_object[key]['published']['date-parts'][0][0], item, json_object[key]['title'])\n",
    "\n",
    "# dict_file        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b43e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = list(pd.read_csv('DOI_all.csv', header=0).fillna(\"\")['doi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6091021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get('https://api.crossref.org/works/10.1016/j.jhydrol.2022.128711')\n",
    "r = requests.get('https://api.crossref.org/works/10.5194/essd-14-5139-2022')\n",
    "r = r.json()['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff924e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "data_url='https://api.openalex.org/works/https://doi.org/{DOI}'\n",
    "response_dict = {}\n",
    "for idx, item in enumerate(set(paper_df)):    \n",
    "    r = requests.get(url = data_url.replace(\"{DOI}\", item))\n",
    "    print(idx)\n",
    "    if r.status_code != 200:\n",
    "        print(r.status_code)\n",
    "    else:\n",
    "        response = r.json()\n",
    "        if response['abstract_inverted_index']:   \n",
    "            abstract = {}         \n",
    "            for key, indexes in response['abstract_inverted_index'].items():\n",
    "                for index in indexes:\n",
    "                    abstract[index] = key\n",
    "            \n",
    "            response_dict[response['doi'].replace('https://doi.org/', '')] = ' '.join(list(collections.OrderedDict(sorted(abstract.items())).values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "792d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abstract.json', 'r') as openfile:     \n",
    "            json_object = json.load(openfile)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5b07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = pd.read_excel(\"GWF_all.xlsx\", header=0).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aad1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', 'doi', '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = re.split(\"([Dd][Oo][Ii])\", 'asdfdoi')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36047db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "doi_list = [None] * len(paper_df)\n",
    "for i in range(len(paper_df)):\n",
    "    publication = paper_df.loc[i][0]\n",
    "    pub = re.split(\"([Dd][Oo][Ii])\", publication)\n",
    "    doi_list[i] = (pub[-1].split()[0].replace('.org/', '') if pub[-1].startswith('.org/') else pub[-1].replace(':', '').split()[0]) if len(pub) >= 3 else None\n",
    "    if len(pub) == 1:\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c4d404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eca8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common parts: []\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "def print_common_parts(file1, file2):\n",
    "    data1 = read_yaml_file(file1)\n",
    "    data2 = read_yaml_file(file2)\n",
    "    \n",
    "    if isinstance(data1, dict) and isinstance(data2, dict):\n",
    "        common_items = {k: v for k, v in data1.items() if k in data2 and data2[k] == v}\n",
    "        print(\"Common parts:\", common_items)\n",
    "    elif isinstance(data1, list) and isinstance(data2, list):\n",
    "        common_elements = [element for element in data1 if element in data2]\n",
    "        print(\"Common parts:\", common_elements)\n",
    "\n",
    "# usage\n",
    "print_common_parts('../data/yaml/name_variants.yaml', 'name_variants.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7f5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "doi_prj_df = pd.read_excel('doi2projects.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082b47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = doi_prj_df['Project Name'].value_counts()\n",
    "prj_dict = {}\n",
    "cnt = 1\n",
    "for item, count in a.iteritems():\n",
    "    prj_dict[item] = \"prj\"+str(cnt)\n",
    "    cnt += 1\n",
    "doi_prj_df['prj'] = doi_prj_df['Project Name'].map(prj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d0a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_prj_df.to_excel('doi2projects1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8a6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"prj1\" \"prj2\" \"prj3\" \"prj4\" \"prj5\" \"prj6\" \"prj7\" \"prj8\" \"prj9\" \"prj10\" \"prj11\" \"prj12\" \"prj13\" \"prj14\" \"prj15\" \"prj16\" \"prj17\" \"prj18\" \"prj19\" \"prj20\" \"prj21\" \"prj22\" \"prj23\" \"prj24\" \"prj25\" \"prj26\" \"prj27\" \"prj28\" \"prj29\" \"prj30\" \"prj31\" \"prj32\" \"prj33\" \"prj34\" \"prj35\" \"prj36\" \"prj37\" \"prj38\" \"prj39\" \"prj40\" \"prj41\" \"prj42\" \"prj43\" \"prj44\" \"prj45\" \n",
      "\"Northern Water Futures\" \"Integrated Modelling Program for Canada (IMPC)\" \"Core Modelling & Forecasting Team\" \"Mountain Water Futures\" \"Core Computer Science Team\" \"Boreal Water Futures: Canada’s Boreal Wildlands-Society-Water Nexus\" \"Sensors and Sensing Systems for Water Quality Monitoring\" \"Agricultural Water Futures\" \"Southern Forests Water Futures\" \"Lake Futures: Enhancing Adaptive Capacity and Resilience of Lakes and their Watersheds\" \"(eDNA) Next Generation Solutions to Ensure Healthy Water Resources for Future Generations\" \"Climate-Related Precipitation Extremes\" \"Transformative Sensor Technologies and Smart Watersheds for Canadian Water Futures\" \"Paradigm Shift in Downscaling Climate Model Projections: Building Models and Tools to Advance Climate Change Research in Cold Regions\" \"Prairie Water: Enhancing resilience of Prairie communities through sustainable water management\" \"FORMBLOOM: Forecasting Tools and Mitigation Options for Diverse Bloom-Affected Lakes\" \"Core Technical Team\" \"Linking Multiple Stressors to Adverse Ecological Responses Across Watersheds\" \"Developing ‘Omic’ and Chemical Fingerprinting Methodologies: Using Ultrahigh-Resolution Mass Spectrometry for Geochemistry and Healthy Waters\" \"Old Meets New: Subsurface Hydrogeological Connectivity and Groundwater Protection\" \"Winter Soil Processes in Transition\" \"SPADE: Storms and Precipitation Across the Continental Divide Experiment\" \"Short‐Duration Extreme Precipitation in Future Climate\" \"GLOBAL WATER CITIZENSHIP: INTEGRATING NETWORKED CITIZENS, SCIENTISTS AND LOCAL DECISION MAKERS\" \"Collaborative Modelling Framework for Water Futures and Holistic Human Health Effects\" \"What is Water Worth? Valuing Canada’s Water Resources and Aquatic Ecosystem Services\" \"Managing Urban Eutrophication Risks under Climate Change: An Integrated Modelling and Decision Support Framework\" \"Linking Stream Network Process Models to Robust Data Management Systems for the Purpose of Land-Use Decision Support\" \"Co-Creation of Indigenous Water Quality Tools\" \"Remotely Sensed Monitoring of Northern lake Ice Using RADARSAT Constellation Mission and Cloud Computing Processing\" \"Data Management \" \"Evaluation of Ice Models in Large Lakes: Using Three-Dimensional Coupled Hydrodynamic-Ice Models\" \"Hydrological Processes in Frozen Soils\" \"Diagnosing and Mitigating Hydrologic Model Uncertainty in High-Latitude Canadian Watersheds\" \"Artificial Intelligence Applications for Rapid and Reliable Detection of Cryptosporidium oocysts and Giardia cysts\" \"SAMMS: Sub-Arctic Metal Mobility Study\" \"Knowledge Mobilization Team\" \"Geogenic contamination of groundwater resources in subarctic regions\" \"Crowdsourcing Water Science\" \"Ohneganos – Indigenous ecological knowledge, training and co-creation of mixed method tools\" \"We need more than just water: Assessing sediment limitation in a large freshwater delta\" \"Significance of Groundwater Dynamics within Hydrologic Models \" \"Matawa Water Futures: Developing an Indigenous-Informed Framework for Watershed Monitoring and Stewardship\" \"Saint John river Experiment on cold Season Storms (SaJESS)\" \"Linking Water Governance in Canada to Global Economic, Social and Political Drivers\" "
     ]
    }
   ],
   "source": [
    "for key, value in prj_dict.items():\n",
    "    print(f'\"{value}\"', end=' ')\n",
    "\n",
    "print()\n",
    "for key, value in prj_dict.items():\n",
    "    print(f'\"{key}\"', end=' ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330cba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34364/2367267143.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  output_df['Project Name'].fillna(\"Others\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "doi_prj_df = pd.read_csv(\"csv_all.csv\")\n",
    "\n",
    "# Select the relevant columns and create a new DataFrame\n",
    "output_df = doi_prj_df[['project', 'doi']].copy()\n",
    "output_df.rename(columns={'project': 'Project Name', 'doi': 'DOI'}, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'Project Name' column with \"Others\"\n",
    "output_df['Project Name'].fillna(\"Others\", inplace=True)\n",
    "\n",
    "output_df['prj'] = \"\"  # Add an empty 'prj' column\n",
    "\n",
    "# Save the output to an Excel file\n",
    "output_df.to_excel('doi2projects.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
